{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e363c3d",
   "metadata": {},
   "source": [
    "# NLP Seminar 3: Static word embeddings (word2vec, fasttext, GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd3f8d4",
   "metadata": {},
   "source": [
    "In this seminar, we will use the `gensim` package, as it has unifying easy-to-use implementations and pretrained word2vec, fasttext, and GloVe models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc88c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64633a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4e07fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dc2b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6638bd1",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a0da84b",
   "metadata": {},
   "source": [
    "Data source: https://www.kaggle.com/datasets/prashant111/the-simpsons-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82dfe53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "      <td>no actually it was a little of both sometimes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "      <td>wheres mr bergstrom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "      <td>i dont know although id sure like to talk to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>That life is worth living.</td>\n",
       "      <td>that life is worth living</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>Springfield Elementary School</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "      <td>the polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text              raw_location_text  \\\n",
       "0              Miss Hoover  Springfield Elementary School   \n",
       "1             Lisa Simpson  Springfield Elementary School   \n",
       "2              Miss Hoover  Springfield Elementary School   \n",
       "3             Lisa Simpson  Springfield Elementary School   \n",
       "4  Edna Krabappel-Flanders  Springfield Elementary School   \n",
       "\n",
       "                                        spoken_words  \\\n",
       "0  No, actually, it was a little of both. Sometim...   \n",
       "1                             Where's Mr. Bergstrom?   \n",
       "2  I don't know. Although I'd sure like to talk t...   \n",
       "3                         That life is worth living.   \n",
       "4  The polls will be open from now until the end ...   \n",
       "\n",
       "                                     normalized_text  \n",
       "0  no actually it was a little of both sometimes ...  \n",
       "1                                wheres mr bergstrom  \n",
       "2  i dont know although id sure like to talk to h...  \n",
       "3                          that life is worth living  \n",
       "4  the polls will be open from now until the end ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpsons = pd.read_csv(\"../data/simpsons_script_lines.csv\",\n",
    "                       usecols=[\"raw_character_text\", \"raw_location_text\", \"spoken_words\", \"normalized_text\"],\n",
    "                       dtype={'raw_character_text':'string', 'raw_location_text':'string',\n",
    "                              'spoken_words':'string', 'normalized_text':'string'})\n",
    "simpsons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad41f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 158271 entries, 0 to 158270\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   raw_character_text  140749 non-null  string\n",
      " 1   raw_location_text   157863 non-null  string\n",
      " 2   spoken_words        132112 non-null  string\n",
      " 3   normalized_text     132087 non-null  string\n",
      "dtypes: string(4)\n",
      "memory usage: 4.8 MB\n"
     ]
    }
   ],
   "source": [
    "simpsons.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b721d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons = simpsons.dropna().drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5dc2b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wheres', 'mr', 'bergstrom']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tok = simpsons['normalized_text'].str.split(' ').to_list()\n",
    "corpus_tok[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e26241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know the Simpsons tv show, \n",
    "# you can e.g. use the wikipedia subset corpus instead,\n",
    "# (and try different words when evaluating the vectors and similarities in next sections):\n",
    "\n",
    "#import gensim.downloader as gensim_api\n",
    "#gensim_api.info('text8')['description']\n",
    "#corpus_tok = gensim_api.load('text8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b207b86",
   "metadata": {},
   "source": [
    "## Phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0174a551",
   "metadata": {},
   "source": [
    "Using a \"phraser\" is a common step before training static word embeddings. \n",
    "It merges together meaningful combinations of tokens, that often appear together in specific contexts and that seem to have a \"meaning\" when used together, into a single token, that one is interested in learning an embedding for. \n",
    "Those \"common phrases\" are also called multi-word expressions, collocations, or word n-grams.\n",
    "\n",
    "For more implementation details: https://radimrehurek.com/gensim/models/phrases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f5f8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, FrozenPhrases, ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "phrases = Phrases(corpus_tok, min_count=30, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "\n",
    "# To save space:\n",
    "phraser = FrozenPhrases(phrases) # or Phraser(phrases) or phrases.freeze()\n",
    "del(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad7400ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['homer_simpson', 'eats', 'donuts']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phraser[[\"homer\", \"simpson\", \"eats\", \"donuts\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffad8b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_phrased = phraser[corpus_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5de72f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wheres', 'mr', 'bergstrom']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_phrased[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a99c2",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61299b3",
   "metadata": {},
   "source": [
    "Word2vec has two sub-methods for training the word embeddings: continuous bag of words (CBOW) and skip-gram.\n",
    "In both cases, a shallow neural network is trained to predict either\n",
    "\n",
    "- a word given a context (CBOW), or\n",
    "- a context of a given a word (skip-gram).\n",
    "\n",
    "The context is defined as the other surrounding words in a given window. The word embedding vectors are then obained from the two trained weight matrices for each word in the vocabulary.\n",
    "\n",
    "\n",
    "Official website: https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "Original papers: http://arxiv.org/abs/1301.3781 and http://arxiv.org/abs/1310.4546"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fab24b",
   "metadata": {},
   "source": [
    "### Training word2vec on the Simpson scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b78c5a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba32471",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_s = Word2Vec(corpus_phrased, vector_size=150, window=3, min_count=2, sg=0, negative=5, ns_exponent=0.75,\n",
    "                 alpha=0.025, min_alpha=0.0001, workers=cores-1, epochs=30)\n",
    "# 1st line: Method's hyperparameters\n",
    "# 2nd line: Optimization (gradient descent) hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf201943",
   "metadata": {},
   "source": [
    "Can also be done in separate steps:\n",
    "```python\n",
    "    w2v_s = Word2Vec(vector_size=150, window=3, min_count=2, sg=0, negative=5, ns_exponent=0.75,\n",
    "                     alpha=0.025, min_alpha=0.0001, workers=cores-1)\n",
    "    w2v_s.build_vocab(sentences, progress_per=10000)\n",
    "    w2v_s.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31a9c3",
   "metadata": {},
   "source": [
    "*Note:* For reproducibility, one must set `workers=1` and a `seed=...`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0366ec2",
   "metadata": {},
   "source": [
    "Word embedding vectors can then be obtained from the trained model, for each word in the training vocabulary. These `KeyedVectors` are stored in the `.wv` argument, and have convenient methods to access and combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e80c7b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x1b5597f89b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_s.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c933ee52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.97504416e-01,  6.43460974e-02,  4.45451364e-02, -2.24762127e-01,\n",
       "        2.18845621e-01, -4.93652187e-02,  3.65614407e-02,  5.74157014e-02,\n",
       "        1.12886772e-01, -1.30046293e-01,  1.45423878e-02, -1.66172273e-02,\n",
       "        5.73144294e-02, -4.79874685e-02,  1.00222528e-01,  1.35900602e-01,\n",
       "       -7.18876347e-02,  5.77266812e-02,  6.15007803e-03, -7.56959245e-02,\n",
       "       -1.40379772e-01, -5.59713803e-02,  8.03122595e-02,  6.45850878e-03,\n",
       "       -1.28210813e-01,  7.50770047e-02,  3.26105542e-02, -8.92766714e-02,\n",
       "       -2.27091219e-02,  6.46338016e-02, -1.46641741e-02,  9.02169049e-02,\n",
       "        1.21256644e-02,  8.66212547e-02, -7.10247606e-02,  7.43433684e-02,\n",
       "       -3.08084220e-01, -3.93914953e-02,  6.38459921e-02, -8.82648379e-02,\n",
       "       -7.35132098e-02,  1.51740210e-02,  1.23751201e-01,  4.52707103e-03,\n",
       "        2.82497145e-02, -1.00846723e-01, -1.15346827e-01, -3.87128443e-02,\n",
       "       -1.58500969e-01,  4.52705510e-02, -4.78413515e-02, -1.30678609e-01,\n",
       "       -4.90224250e-02, -7.95665085e-02,  4.01428677e-02, -2.49620015e-03,\n",
       "        4.85728905e-02, -1.20143378e-02,  4.07003872e-02, -1.42947197e-01,\n",
       "        1.03972666e-03,  2.47136038e-03, -7.68941315e-03,  5.68625145e-02,\n",
       "        2.52312701e-02, -2.05736756e-02, -6.45712689e-02, -2.65517645e-02,\n",
       "        1.33054778e-01, -2.79959161e-02, -5.18476292e-02, -6.50102347e-02,\n",
       "       -3.26361395e-02, -1.09576754e-01, -9.68294218e-02,  2.14125738e-02,\n",
       "       -3.63899991e-02, -1.21850654e-01,  1.03238478e-01,  1.59077672e-03,\n",
       "       -2.73707956e-02,  3.55852535e-03, -4.74180430e-02, -1.67649299e-01,\n",
       "       -7.32694473e-03, -1.39486147e-02, -7.36500919e-02, -8.09154287e-02,\n",
       "       -9.69211683e-02,  2.64922250e-02,  1.31861344e-01, -4.17544357e-02,\n",
       "        3.92968580e-02,  7.42383813e-03,  4.53827158e-02, -1.56306196e-02,\n",
       "        9.51587036e-02, -2.42334530e-02, -5.90158217e-02, -2.00419445e-02,\n",
       "       -5.09008067e-03, -3.84392701e-02, -5.67794070e-02, -1.68018669e-01,\n",
       "       -5.73033355e-02,  2.54590828e-02,  2.47195875e-03, -5.80890179e-02,\n",
       "       -4.09669913e-02,  7.38284886e-02,  3.27137709e-02,  7.84575120e-02,\n",
       "       -1.00819170e-02,  1.08360322e-02,  8.08227509e-02,  9.69527811e-02,\n",
       "        3.06299143e-02, -1.17891915e-01,  4.84264875e-03,  2.23419622e-01,\n",
       "        6.68756738e-02,  7.38208592e-02,  7.72299021e-02, -1.10221609e-01,\n",
       "        4.70762067e-02, -8.87180865e-02,  1.72741385e-03,  6.37067407e-02,\n",
       "       -2.29958538e-02,  7.88405631e-03,  7.25841522e-03,  2.67557334e-02,\n",
       "       -6.19423948e-02,  8.79978463e-02, -2.12996092e-05,  1.02674542e-02,\n",
       "        1.47822099e-02,  7.17610819e-03,  1.16927207e-01, -2.81864051e-02,\n",
       "        6.05125539e-02, -6.72654808e-02, -7.15225795e-03,  1.65870279e-01,\n",
       "       -7.14600459e-02,  4.91301008e-02, -5.17349727e-02,  7.57192373e-02,\n",
       "        7.32252980e-03,  1.04812071e-01], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homer_vector = w2v_s.wv.get_vector(\"homer\", norm=True) # Father of the Simpsons\n",
    "homer_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6ccba",
   "metadata": {},
   "source": [
    "For a given word or vector, one can query the other most similar word vectors, in terms of cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c8fcd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marge', 0.6317975521087646),\n",
       " ('homie', 0.5999241471290588),\n",
       " ('bart', 0.5930087566375732),\n",
       " ('dad', 0.5614809393882751),\n",
       " ('son', 0.5609906911849976),\n",
       " ('moe', 0.549460232257843),\n",
       " ('ned', 0.5298517942428589),\n",
       " ('grampa', 0.5063692927360535),\n",
       " ('mom', 0.5041996836662292),\n",
       " ('honey', 0.4845598340034485)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_s.wv.most_similar(\"homer\") # Marge is Homer's wife and Bart is his son. Homer is the \"dad\" of the family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b0b648a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('homer', 1.0),\n",
       " ('marge', 0.6317975521087646),\n",
       " ('homie', 0.5999241471290588),\n",
       " ('bart', 0.5930087566375732),\n",
       " ('dad', 0.5614809393882751),\n",
       " ('son', 0.5609906911849976),\n",
       " ('moe', 0.549460232257843),\n",
       " ('ned', 0.5298517942428589),\n",
       " ('grampa', 0.5063692927360535),\n",
       " ('mom', 0.5041996836662292)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_s.wv.most_similar(homer_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22a9c52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('manager', 0.4670054316520691),\n",
       " ('ned_flanders', 0.45402565598487854),\n",
       " ('montgomery_burns', 0.4459589719772339),\n",
       " ('abraham', 0.43679898977279663),\n",
       " ('bart_simpson', 0.43056923151016235),\n",
       " ('robert', 0.4298505485057831),\n",
       " ('chester', 0.42625194787979126),\n",
       " ('kent_brockman', 0.4187189042568207),\n",
       " ('johnson', 0.4120151698589325),\n",
       " ('mayor', 0.40999096632003784)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_s.wv.most_similar(\"homer_simpson\") # name bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67437c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.6728383898735046),\n",
       " ('milhouse', 0.6387649774551392),\n",
       " ('dad', 0.5937414765357971),\n",
       " ('homer', 0.5930086970329285),\n",
       " ('son', 0.5514666438102722),\n",
       " ('mom', 0.5388515591621399),\n",
       " ('your_father', 0.5289860963821411),\n",
       " ('honey', 0.5228714346885681),\n",
       " ('principal_skinner', 0.4911167025566101),\n",
       " ('homie', 0.4858196973800659)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_s.wv.most_similar(\"bart\") # Bart is the son, Lisa his sister and Milhouse his best friend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d2dfa",
   "metadata": {},
   "source": [
    "One can also compute the cosine similarity between two word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b8a94fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6728384"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_s.wv.similarity('bart', 'lisa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acff6c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999994"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_s.wv.similarity('bart', 'bart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7348775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46287888"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_s.wv.n_similarity([\"marge\", \"homer\"], [\"wife\", \"husband\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca51aad",
   "metadata": {},
   "source": [
    "Odd-one-out identification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e05bde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'homer'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_s.wv.doesnt_match(['homer', 'patty', 'selma']) # Patty and Selma are Marge's twin sisters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc0f5e",
   "metadata": {},
   "source": [
    "Word analogies: how well do embeddings vectors capture intuitive semantic and syntactic analogy questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d13bddc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marge', 0.5275154709815979),\n",
       " ('mom', 0.5146074891090393),\n",
       " ('homie', 0.5048220753669739)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \" Homer - man + woman = ? \" - i.e. \" man:Homer :: woman:? \"\n",
    "w2v_s.wv.most_similar(positive=[\"homer\", \"woman\"], negative=[\"man\"], topn=3) # Marge is Homer's wife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55155190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.47464245557785034),\n",
       " ('friend', 0.4618160128593445),\n",
       " ('child', 0.438487708568573)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \" woman - Marge + Homer = ? \" - i.e. \" Marge:Homer :: woman:? \" \n",
    "w2v_s.wv.most_similar(positive=[\"woman\", \"homer\"], negative=[\"marge\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f30f4",
   "metadata": {},
   "source": [
    "### Sentence embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b7297b",
   "metadata": {},
   "source": [
    "A sentence or document embedding can, for example, be obtained by averaging its tokens' embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ca96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document2vec(tokens, embedding_wv, phraser=None, normalize=True):\n",
    "    \"\"\"Returns the embedding of a sentence or document as the mean of its tokens/words embeddings.\"\"\"\n",
    "    if phraser:\n",
    "        tokens = phraser[tokens]\n",
    "    # sent_mean = np.array([embedding_wv.get_vector(tok, norm=normalize) for tok in tokens]).mean(axis=0) # same result\n",
    "    sent_mean = embedding_wv.get_mean_vector(keys=tokens, ignore_missing=True) # same result (robust to OOV words)\n",
    "    return sent_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77703262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.009877  ,  0.01109686, -0.00165891, -0.017362  ,  0.05698981,\n",
       "       -0.02443988, -0.03313335,  0.05180913, -0.01051246,  0.00570679,\n",
       "        0.00610514, -0.01178046,  0.00710768,  0.05084235,  0.00538733,\n",
       "        0.04277755, -0.05651882,  0.00355378, -0.04752327,  0.04393594,\n",
       "       -0.04326501, -0.01511771,  0.00783788,  0.00466334, -0.01279658,\n",
       "       -0.04956766, -0.02699601, -0.0048399 ,  0.04675311,  0.00818683,\n",
       "       -0.01231978, -0.03479793, -0.00063789,  0.04933694, -0.00058257,\n",
       "        0.09272969,  0.02091086,  0.04867287,  0.04694089,  0.023596  ,\n",
       "       -0.00434448,  0.03485893,  0.03434121,  0.01915275,  0.02651068,\n",
       "        0.02080218, -0.04208039, -0.01016911,  0.03299143,  0.00735796,\n",
       "        0.0140503 , -0.01229267,  0.09844785, -0.05196367,  0.09528995,\n",
       "        0.05697097, -0.05021961, -0.02764725,  0.06670328,  0.02159331,\n",
       "        0.04906306, -0.03101779,  0.00488023, -0.04220416,  0.01963334,\n",
       "       -0.05249187, -0.03727938,  0.04126221,  0.03961276,  0.0035751 ,\n",
       "       -0.0312985 , -0.04712046,  0.02706213, -0.02850389, -0.00410944,\n",
       "        0.07369407,  0.02052266, -0.07966556,  0.09082342,  0.01869388,\n",
       "       -0.04411286,  0.04871587,  0.06849717, -0.04917287, -0.0020274 ,\n",
       "        0.08256426, -0.00080734,  0.00928072, -0.0752788 , -0.04841873,\n",
       "       -0.02774334, -0.07518288, -0.03196163, -0.0068481 , -0.01378327,\n",
       "        0.06288281,  0.0152073 ,  0.05820203, -0.01828608,  0.00468643,\n",
       "        0.0037047 ,  0.07097596, -0.07394918, -0.01495785,  0.00618478,\n",
       "        0.08766442,  0.09829114,  0.00454202, -0.04255578,  0.05107681,\n",
       "        0.05598912, -0.02888563, -0.00940105,  0.08006164,  0.07277066,\n",
       "        0.07155251, -0.00425195, -0.0437894 ,  0.03026315, -0.02051809,\n",
       "       -0.02431684, -0.01283352,  0.00835041,  0.04319839, -0.00336432,\n",
       "        0.01082545,  0.0760375 , -0.08316918, -0.0515427 ,  0.11103813,\n",
       "       -0.01058708,  0.01557367, -0.05343645, -0.04100156,  0.0322227 ,\n",
       "       -0.02628685,  0.01581221,  0.05655019, -0.05825509, -0.00221902,\n",
       "        0.00264369, -0.01149607, -0.06818324,  0.05279033,  0.0552939 ,\n",
       "        0.03113272, -0.06648487,  0.0308219 ,  0.04466842, -0.06447339],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document2vec([\"bart\", \"is\", \"grounded\"], w2v_s.wv, phraser=phraser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae21d85",
   "metadata": {},
   "source": [
    "### Pretrained word2vec vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f07e270",
   "metadata": {},
   "source": [
    "https://github.com/RaRe-Technologies/gensim-data#models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cea08609",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as gensim_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4842ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_pret = gensim_api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca0624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or from downloaded source (e.g. https://code.google.com/archive/p/word2vec/):\n",
    "#w2v_pret = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8cf6e802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x2b7809449e0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_pret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7ea335e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eating', 0.7529403567314148),\n",
       " ('ate', 0.7013993859291077),\n",
       " ('eaten', 0.6724975109100342),\n",
       " ('eats', 0.6589087843894958),\n",
       " ('munch', 0.6417747735977173),\n",
       " ('eat_healthfully', 0.6315395832061768),\n",
       " ('eat_fatty_foods', 0.6280142068862915),\n",
       " ('consume', 0.6184970140457153),\n",
       " ('Nutritionists_recommend', 0.6183844804763794),\n",
       " ('overeaten', 0.6109130382537842)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_pret.most_similar(positive=[\"eat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de4fa1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6184971"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_pret.similarity(\"eat\", 'consume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4caefd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dance'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_pret.doesnt_match([\"eat\", 'dance', 'drink'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "525f3956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_pret.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ab33091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_pret.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8398d4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.7992597818374634),\n",
       " ('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321839332581),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.518113374710083),\n",
       " ('sultan', 0.5098593831062317)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectcalc = w2v_pret.get_vector(\"king\", norm=True) - w2v_pret.get_vector(\"man\", norm=True) + w2v_pret.get_vector(\"woman\", norm=True)\n",
    "w2v_pret.most_similar(vectcalc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61dbb06",
   "metadata": {},
   "source": [
    "king - man + woman = king ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d84f05ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('joseph', 0.620844841003418),\n",
       " ('boston_globe', 0.6205891370773315),\n",
       " ('liz', 0.6189237833023071),\n",
       " ('gerald', 0.6109275817871094),\n",
       " ('jta', 0.6039943099021912),\n",
       " ('jg', 0.6032029986381531),\n",
       " ('meyers', 0.6026628613471985),\n",
       " ('ellis', 0.5999051928520203),\n",
       " ('christine', 0.5992782115936279),\n",
       " ('becky', 0.5988665223121643)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_pret.most_similar(positive=[\"bart\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "435548b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.7069635987281799),\n",
       " ('milhouse', 0.6307767629623413),\n",
       " ('dad', 0.5766920447349548),\n",
       " ('homer', 0.5670747756958008),\n",
       " ('mom', 0.5558942556381226),\n",
       " ('son', 0.5200268030166626),\n",
       " ('your_father', 0.5010845065116882),\n",
       " ('honey', 0.49810099601745605),\n",
       " ('you', 0.4977051317691803),\n",
       " ('maggie', 0.4751102328300476)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_s.wv.most_similar(\"bart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72e10b",
   "metadata": {},
   "source": [
    "# Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8e09bf",
   "metadata": {},
   "source": [
    "Fastext is a static word embedding, that is very similar to word2vec. As a main difference, it uses character-level ngram vectors together with the word vectors.\n",
    "\n",
    "Advantages:\n",
    "+ Out of training vocabulary embeddings are obtainable.\n",
    "+ Better representation for rare words (that are morphologically similar to others).\n",
    "+ Tends to perform better for syntactic tasks.\n",
    "+ Is more useful in morphologically rich languages (such as German, Arabic and Russian) compared to English (German example: 'table tennis' -> 'Tischtennis'), but it heavily depends on the task.\n",
    "+ might work better for small datasets.\n",
    "+ The \"official\" implementation is quite efficient, and allows training the embedding and classifier at once (see the official `fasttest` package documentation https://fasttext.cc/docs/en/python-module.html).\n",
    "\n",
    "Disatvantages:\n",
    "- Can overfit more easily, and is a bit harder to fine tune with the additionnal character ngram hyperparameters.\n",
    "- Tends to perform more poorly for semantic tasks.\n",
    "- May tend to privilege too much the morphologically close synonyms compared to other semantically closer synomyms.\n",
    "- Can be heavier to train.\n",
    "\n",
    "However, the differences between fasttext and word2vec thend to decrease as the size of the training corpus increases.\n",
    "\n",
    "Official website: https://fasttext.cc/\n",
    "\n",
    "Original paper: https://arxiv.org/abs/1607.04606"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd1cc7",
   "metadata": {},
   "source": [
    "E.g.: for a charactrer-ngram window of n=4, it uses the average of vectors for \"university\", \"univ\", \"nive\", ... , \"sity\" as input representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2a656",
   "metadata": {},
   "source": [
    "### Training fasttext on the Simpson scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fe7c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3685e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_s = FastText(corpus_phrased, vector_size=150, window=3, min_count=5, sg=0, negative=5, ns_exponent=0.75,\n",
    "                 min_n=3, max_n=6, # Additional fasttext hyperparameters\n",
    "                 alpha=0.025, min_alpha=0.0001, workers=cores-1, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0ce61e",
   "metadata": {},
   "source": [
    "Can also be performed in separate steps:\n",
    "```python\n",
    "    fst_s = FastText(vector_size=150, window=3, min_count=5, sg=0, negative=5, ns_exponent=0.75,\n",
    "                     min_n = 1, max_n = 4,\n",
    "                     alpha=0.025, min_alpha=0.0001, workers=cores-1)\n",
    "    fst_s.build_vocab(corpus_phrased)\n",
    "    print(len(fst_s.wv.vocab.keys()))\n",
    "    fst_s.train(sentences, total_examples = fst_s.corpus_count, epochs=100) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab114219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'you', 'i', 'a', 'to', 'and', 'of', 'it', 'that', 'in', 'is', 'my', 'this', 'for', 'your', 'me', 'on', 'oh', 'we', 'im', 'have', 'but', 'what', 'no', 'well', 'its', 'just', 'with', 'do', 'are', 'now', 'not', 'be', 'was', 'all', 'so', 'get', 'can', 'youre', 'dont', 'like', 'one', 'at', 'thats', 'hey', 'here', 'out', 'if', 'know', 'up', 'he', 'homer', 'were', '--', 'our', 'go', 'from', 'bart', 'they', 'ill', 'yeah', 'there', 'about', 'think', 'how', 'want', 'an', 'right', 'as', 'look', 'see', 'marge', 'good', 'got', 'uh', 'dad', 'okay', 'him', 'when', 'back', 'will', 'some', 'cant', 'little', 'us', 'man', 'could', 'time', 'come', 'who', 'did', 'his', 'say', 'why', 'would', 'hes', 'or', 'take', 'by', 'make']\n"
     ]
    }
   ],
   "source": [
    "print(fst_s.wv.index_to_key[:100]) # First 100 (most frequent) words in the model vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1661375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"unige\" in fst_s.wv.index_to_key, \"unige\" in w2v_s.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da69fcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: the given token is not not present in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(w2v_s.wv.get_vector(\"unige\", norm=True))\n",
    "except:\n",
    "    print(\"KeyError: the given token is not not present in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ff45ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01608711  0.04110799  0.17988564 -0.01597484  0.09143241  0.0618559\n",
      " -0.10572785  0.04773679  0.06128706  0.08347788 -0.12178781  0.00537748\n",
      "  0.01148988  0.05912064 -0.14781815 -0.01172447  0.03322294 -0.06930365\n",
      "  0.14204437 -0.0088919  -0.01566534 -0.09653875  0.05829462  0.14226532\n",
      "  0.04213895  0.17472796  0.06464078 -0.03843708  0.1057763  -0.11764871\n",
      " -0.0761048  -0.10572392 -0.0178628  -0.10062825  0.22040248  0.11360672\n",
      " -0.07555386 -0.02123816 -0.07728361 -0.05543618 -0.14223951  0.07038567\n",
      "  0.01215003 -0.08780414  0.0427446   0.03708046 -0.12724902 -0.13829008\n",
      " -0.09344315  0.12672943 -0.05068863  0.06170019 -0.06100393  0.1073439\n",
      " -0.16181694 -0.03976557 -0.00716341  0.09881999  0.0078082  -0.04061393\n",
      " -0.06061785 -0.08024641 -0.03772819  0.01696859  0.10869474 -0.00998171\n",
      "  0.02471528  0.07926501  0.03131536  0.07140689  0.03026498 -0.0235145\n",
      "  0.00374927  0.04195894  0.05051354 -0.07675543 -0.00371963  0.05776955\n",
      " -0.0427718  -0.06232553  0.01374752 -0.05637484 -0.0418407   0.07471007\n",
      "  0.01366204 -0.04962629 -0.00125682  0.19154522 -0.04844473 -0.00163942\n",
      "  0.10534021 -0.04988391 -0.04039003 -0.07416814 -0.09461623 -0.04812187\n",
      "  0.02563489  0.0231753  -0.00973709 -0.02166246  0.02433282 -0.05201336\n",
      "  0.04559216  0.01351663  0.01783666 -0.149012    0.13692117 -0.06054755\n",
      "  0.16400386 -0.01200296  0.13034755 -0.03877532  0.09268129  0.08310092\n",
      "  0.03239707  0.03880267  0.04828462 -0.01533471  0.08919001 -0.02959738\n",
      " -0.0048472  -0.12032568  0.02759641 -0.15525194  0.154746    0.04885809\n",
      " -0.18400995 -0.01369562 -0.1874332  -0.09944628  0.08137952  0.01314117\n",
      "  0.12963302 -0.08517756 -0.0586291   0.15561403  0.03030139  0.08592445\n",
      "  0.06911021  0.00819645 -0.00233383 -0.02872962  0.0582826   0.00446454\n",
      "  0.03293407 -0.00386611 -0.00524056 -0.02998714  0.11022492 -0.03074474]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(fst_s.wv.get_vector(\"unige\", norm=True))\n",
    "except:\n",
    "    print(\"KeyError: the given token is not not present in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e289f1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unit', 0.8625775575637817),\n",
       " ('unite', 0.8335436582565308),\n",
       " ('university', 0.8153852820396423),\n",
       " ('unique', 0.8051110506057739),\n",
       " ('universe', 0.796246349811554),\n",
       " ('union', 0.7779499888420105),\n",
       " ('universal', 0.7628664970397949),\n",
       " ('un', 0.7502937316894531),\n",
       " ('uniform', 0.7458596229553223),\n",
       " ('unemployment', 0.7442814707756042)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fst_s.wv.most_similar(\"unige\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65c1e320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('homey', 0.8086385726928711),\n",
       " ('knockahomer', 0.7412236928939819),\n",
       " ('homeboy', 0.7314950823783875),\n",
       " ('homers', 0.7251979112625122),\n",
       " ('homer_simpson', 0.7221113443374634),\n",
       " ('homemaker', 0.7107552886009216),\n",
       " ('homie', 0.6970348954200745),\n",
       " ('marge', 0.6948388814926147),\n",
       " ('bart', 0.6739369630813599),\n",
       " ('customer', 0.6527592539787292)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fst_s.wv.most_similar(\"homer\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "699e5aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('maaarge', 0.8323073387145996),\n",
       " ('margie', 0.809052050113678),\n",
       " ('sarge', 0.7575308680534363),\n",
       " ('marge-a-rine', 0.7560793161392212),\n",
       " ('margaret', 0.7486603260040283),\n",
       " ('marges', 0.748385488986969),\n",
       " ('margarita', 0.7326395511627197),\n",
       " ('marie', 0.695029079914093),\n",
       " ('homer', 0.69483882188797),\n",
       " ('marjorie', 0.6896591186523438)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fst_s.wv.most_similar(\"marge\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc1181a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abe', 0.5942814946174622),\n",
       " ('homer', 0.5923839211463928),\n",
       " ('homie', 0.5613185167312622),\n",
       " ('honey', 0.5219800472259521),\n",
       " ('lisa', 0.51099693775177),\n",
       " ('maggie', 0.5003818869590759),\n",
       " ('but', 0.48739856481552124),\n",
       " ('mom', 0.48072031140327454),\n",
       " ('you', 0.4766426682472229),\n",
       " ('well', 0.4755488932132721)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_s.wv.most_similar(\"marge\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1250e569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('teat', 0.7086275219917297),\n",
       " ('neat', 0.6601749062538147),\n",
       " ('beat', 0.6542387008666992),\n",
       " ('earn', 0.6485193967819214),\n",
       " ('meat', 0.6215378642082214),\n",
       " ('drink', 0.6123024821281433),\n",
       " ('eats', 0.6090422868728638),\n",
       " ('sweat', 0.6041677594184875),\n",
       " ('eatin', 0.6033998131752014),\n",
       " ('ear', 0.5854046940803528)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fst_s.wv.most_similar(\"eat\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8fc77d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('drink', 0.5787950158119202),\n",
       " ('feed', 0.5256111025810242),\n",
       " ('buy', 0.5093466639518738),\n",
       " ('steal', 0.49575188755989075),\n",
       " ('die', 0.4946196675300598),\n",
       " ('throw', 0.48809781670570374),\n",
       " ('sell', 0.486087828874588),\n",
       " ('lose', 0.4823223054409027),\n",
       " ('wear', 0.4689778983592987),\n",
       " ('scrape', 0.46692678332328796)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_s.wv.most_similar(\"eat\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394fa992",
   "metadata": {},
   "source": [
    "### Pretrained fasttext vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5eb1fa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_pret = gensim_api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce25f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or from downloaded source (e.g. https://fasttext.cc/docs/en/english-vectors.html):\n",
    "# fst_pret = FastText.load_fasttext_format('fasttest_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "38ac3b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eate', 0.80073082447052),\n",
       " ('eat-', 0.7924754619598389),\n",
       " ('eatin', 0.7710843682289124),\n",
       " ('eating', 0.7475292682647705),\n",
       " ('ate', 0.739423930644989),\n",
       " ('eat.', 0.7381978631019592),\n",
       " ('eats', 0.7242209315299988),\n",
       " ('consume', 0.7190532088279724),\n",
       " ('eaten', 0.7079055905342102),\n",
       " ('eatable', 0.7070158123970032)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fst_pret.most_similar(\"eat\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bdce5fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eating', 0.7529403567314148),\n",
       " ('ate', 0.7013993859291077),\n",
       " ('eaten', 0.6724975109100342),\n",
       " ('eats', 0.6589087843894958),\n",
       " ('munch', 0.6417747735977173),\n",
       " ('eat_healthfully', 0.6315395832061768),\n",
       " ('eat_fatty_foods', 0.6280142068862915),\n",
       " ('consume', 0.6184970140457153),\n",
       " ('Nutritionists_recommend', 0.6183844804763794),\n",
       " ('overeaten', 0.6109130382537842)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_pret.most_similar(\"eat\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c3c92413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('consumes', 0.7680187821388245),\n",
       " ('over-consume', 0.7483881711959839),\n",
       " ('consumed', 0.7472653388977051),\n",
       " ('consuming', 0.7410412430763245),\n",
       " ('eat', 0.7190532088279724),\n",
       " ('overconsume', 0.6897545456886292),\n",
       " ('devour', 0.6756592392921448),\n",
       " ('ingest', 0.65708327293396),\n",
       " ('Consume', 0.6442354917526245),\n",
       " ('consumption', 0.6395013928413391)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fst_pret.most_similar(\"consume\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a52b166e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('consumed', 0.698164701461792),\n",
       " ('consumes', 0.6695359349250793),\n",
       " ('eat', 0.6184970736503601),\n",
       " ('consumption', 0.6043094992637634),\n",
       " ('guzzle', 0.5901384949684143),\n",
       " ('ingest', 0.5877813696861267),\n",
       " ('consuming', 0.5756837725639343),\n",
       " ('overconsume', 0.5606318712234497),\n",
       " ('devour', 0.5577657222747803),\n",
       " ('Consuming', 0.5523046851158142)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_pret.most_similar(\"consume\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9f974",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442cfa50",
   "metadata": {},
   "source": [
    "Contrary to word2vec and fasttext, GloVe doesn't use skipgram or CBOW networks. GloVe relies on the word-context co-occurrence matrix to obtain the embedded word vectors.\n",
    "\n",
    "- GloVe can be longer to train on larger corpora, compared to word2vec.\n",
    "- It has fewer hyperparameters, so it's much easier to tune, but then cannot be fine tuned for a specific task.\n",
    "- word2vec and fasttext are in comparison much more sensitive to the coices of hyperparameters, and results can thus vary much more.\n",
    "\n",
    "Official website: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Original paper: https://nlp.stanford.edu/pubs/glove.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9f4d3",
   "metadata": {},
   "source": [
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b63fb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glv_pret = gensim_api.load(\"glove-wiki-gigaword-200\")\n",
    "\n",
    "# Are already available in gensim:\n",
    "#'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300',\n",
    "#'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294cb5f",
   "metadata": {},
   "source": [
    "From downloaded source (e.g. https://nlp.stanford.edu/projects/glove/), one can do:\n",
    "```python\n",
    "    from gensim.test.utils import datapath, get_tmpfile\n",
    "    from gensim.models import KeyedVectors\n",
    "    from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "    glove_file = datapath('DOWNLOADED_GLOVE_VECTORS.txt')\n",
    "    tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "    glove2word2vec(glove_file, tmp_file)\n",
    "    model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "02bac8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('faster', 0.7852828502655029),\n",
       " ('slow', 0.6931735873222351),\n",
       " ('slower', 0.6770131587982178)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glv_pret.most_similar(positive=[\"better\", \"fast\"], negative=[\"good\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "97a9c62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eating', 0.7841552495956421),\n",
       " ('ate', 0.7657052874565125),\n",
       " ('eaten', 0.7538666129112244),\n",
       " ('meal', 0.6805590987205505),\n",
       " ('consume', 0.6571250557899475),\n",
       " ('eats', 0.6406125426292419),\n",
       " ('food', 0.6227813363075256),\n",
       " ('meat', 0.6211603879928589),\n",
       " ('drink', 0.6211259961128235),\n",
       " ('vegetables', 0.6168597340583801)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glv_pret.most_similar(\"eat\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6d253f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('consumed', 0.7226211428642273),\n",
       " ('consumes', 0.6638200283050537),\n",
       " ('eat', 0.657124936580658),\n",
       " ('consuming', 0.6452376842498779),\n",
       " ('devour', 0.5809320211410522),\n",
       " ('consumption', 0.5808643698692322),\n",
       " ('feed', 0.5677404999732971),\n",
       " ('ingest', 0.5554704666137695),\n",
       " ('calories', 0.5538193583488464),\n",
       " ('drink', 0.5402347445487976)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glv_pret.most_similar(\"consume\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a4491",
   "metadata": {},
   "source": [
    "### Remark: training GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8416f5",
   "metadata": {},
   "source": [
    "Training GloVe vectors is not possible with gensim. If interested, one can use the [official GloVe code](https://nlp.stanford.edu/projects/glove/) (command line interface).\n",
    "\n",
    "For a python interface, see for example the (\"toy implementation\") [`glove_python`](https://github.com/maciejkula/glove-python) pachage\n",
    "\n",
    "    !pip install glove_python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d4a4b",
   "metadata": {},
   "source": [
    "### See also other vector embeddings..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1217d0",
   "metadata": {},
   "source": [
    "https://github.com/RaRe-Technologies/gensim-data#models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "13734dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_api.info('conceptnet-numberbatch-17-06-300')['description']\n",
    "#conceptnet = gensim_api.load(\"conceptnet-numberbatch-17-06-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2d347",
   "metadata": {},
   "source": [
    "# Saving gensim models and word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9a923",
   "metadata": {},
   "source": [
    "One can save either the entire model (if further training is expected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_s.save('./word2vec_simpson_model')\n",
    "w2v_s = Word2Vec.load('./word2vec_simpson_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac40a19a",
   "metadata": {},
   "source": [
    "Or only the word vectors (the `KeyedVectors`-type attribute) if the vecors are final. They are much more memory-efficient to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a64dadde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2v_s.wv.save('word2vec_simpson_word_vectors')\n",
    "w2v_s_wv = KeyedVectors.load('word2vec_simpson_word_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2347d1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.7069635987281799),\n",
       " ('milhouse', 0.6307767629623413),\n",
       " ('dad', 0.5766920447349548),\n",
       " ('homer', 0.5670747756958008),\n",
       " ('mom', 0.5558942556381226),\n",
       " ('son', 0.5200268030166626),\n",
       " ('your_father', 0.5010845065116882),\n",
       " ('honey', 0.49810099601745605),\n",
       " ('you', 0.4977051317691803),\n",
       " ('maggie', 0.4751102328300476)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_s_wv.most_similar(\"bart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9671a4f9",
   "metadata": {},
   "source": [
    "# Exercise: ML classification using advanced static embeddings\n",
    "\n",
    "Compare the performance of the logistic regression classifier on the 20newsgroup dataset using word2vect, GloVe or fasttext to the performance achieved in the previous seminar using TF-IDF."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
