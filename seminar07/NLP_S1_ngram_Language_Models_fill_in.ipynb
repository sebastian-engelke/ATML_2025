{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65559326e76d2b7841244e1dd6ea448671411b95"
   },
   "source": [
    "# NLP Seminar 1: N-grams Language Models\n",
    "\n",
    "In this NLP seminar, we will learn to estimate and use n-gram language models (LM). They are a classical approach to sentence modelling, and text autocompletion.\n",
    "\n",
    "We will use the `nltk` (natural language toolkit) python package. \n",
    "If you want to learn more about this popular module, refer to the [official website](https://www.nltk.org/) ([API reference](https://www.nltk.org/api/nltk.html), [installation guide](https://www.nltk.org/install.html)).\n",
    "\n",
    "In particular, the `nltk.lm` submodule provides optimized implementations of classical n-grams language models such as the maximum likelihood estimator (MLE) and its smoothing variants (Laplace, Lidstone, ...).\n",
    "\n",
    "To illustrate the ngram approach, we will use n-gram LMs (LM) to model script lines (what the characters say) from the Simpsons TV show. The goal will then be to generate new script lines, or do autocompletion, in the writing style of The Simpsons. \n",
    "\n",
    "Before that, we will begin with the basics on how to preprocess the text data into tokens and ngrams, which are a prerequisite step for fitting those LMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b213fb0dd5534ce82d6f1c716e9695ba5cf9758"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First download some nltk resources\n",
    "# (By default 'pip install nltk' does not actually download every resource in the module,\n",
    "# as for example some language models are heavy.)\n",
    "# The following commands should download every resource needed for this practical:\n",
    "nltk.download('popular', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction: preprocessing and n-grams with dummy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we consider the dummy corpus `corp` with two tokenized documents (sequences of tokens). The tokens are here simple letters, but we can think of them as representing words in our vocabulary. (A raw corpus would need to be tokenized first.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = [['a', 'b', 'c'], ['a', 'c', 'd', 'c', 'e', 'f']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to train a bigram model, we need to turn this tokenized text into n-grams. We can use the `bigrams` and `ngrams` functions from NLTK as helpers, to turn the each token list document into an ngram list (for ex. with $n=2$ and $n=3$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bigrams(?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ngrams(?, n=?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remark:* The `list()` is here just used to display the results, as `bigrams`, `ngrams` and other `nltk` functions return python lazy generators, for efficiency.\n",
    "\n",
    "Notice how \"b\" occurs both as the first and second member of different bigrams but \"a\" and \"c\" don't? \n",
    "\n",
    "It would be nice to indicate to the model how often sentences start with \"a\" and end with \"c\" for example, when we will count those ngrams later-on.\n",
    "\n",
    "\n",
    "A standard way to deal with this is to add special \"padding\" symbols to the document/sequence before splitting it into ngrams. Fortunately, NLTK also has a `pad_sequence` function for that. We use `\"<s>\"` and `\"</s>\"` by convention in `nltk` to pad before and after the sequence, respectively.\n",
    "\n",
    "Lets add the relevent paddings and construct the bigrams and 3-grams for the first text sequence. Note the `n` argument, that tells the function we need padding for `n`-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n=2\n",
    "padded_seq2 = list(pad_sequence(?, n=?, # n: order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc. \n",
    "                                pad_left=?, left_pad_symbol=?,\n",
    "                                pad_right=?, right_pad_symbol=?)\n",
    "                   )\n",
    "padded_seq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ngrams(?, n=?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n=3\n",
    "padded_seq3 = list(pad_sequence(corp[0], n=3, # n: order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc. \n",
    "                                pad_left=True, left_pad_symbol=\"<s>\",\n",
    "                                pad_right=True, right_pad_symbol=\"</s>\")\n",
    "                   )\n",
    "padded_seq3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ngrams(padded_seq3, n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing all these parameters every time can be tedious and in most cases one uses the same defaults anyway.\n",
    "\n",
    "Thus, the `nltk.lm` module provides a convenience function that has all these arguments already set while the other arguments remain the same as for `pad_sequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pad_both_ends(corp[0], n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the two parts discussed so far we get the following preparation steps for one sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bigrams(pad_both_ends(corp[0], n=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For versatility and conditional probability computations, the `nltk.lm` n-gram models that we will use typically rely on counting everygrams of order n. \n",
    "For example, LMs of order 2 are trained by counting unigrams (single words) as well as bigrams (word pairs). For LMs of order 3, they usually rely on counting unigrams, bigrams and 3-grams. And so on... \n",
    "That way, an `nltk` LM model of order $n$ can output word probabilities for contexts (i.e. previous words/tokens in the conditioning) of size $0, 1, 2, ..., n-1$ tokens.\n",
    "\n",
    "To construct those everygrams, that will serve as training data for the LM model to count, NLTK once again helpfully provides a function called `everygrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import everygrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_seq2 = list(pad_both_ends(corp[0], n=2))\n",
    "\n",
    "list(everygrams(padded_seq2, max_len=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to start counting ngrams, just one more step left.\n",
    "\n",
    "During training and evaluation our model will rely on a vocabulary that defines which words are \"known\" to the model, to efficiently perform and store the word and ngram counts.\n",
    "\n",
    "One can create this vocabulary we need to pad our sentences (just like for counting ngrams) and then combine the sentences into one flat stream of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(flatten(pad_both_ends(sent, n=2) for sent in corp)) #vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we discussed the necessary preprocessing steps, in most cases, one typically wants to use the same text as the source for both vocabulary and ngram counts.\n",
    "\n",
    "To this aim, the `padded_everygram_pipeline` function does exactly everything above (padding, everygrams, vocabulary stream) for us for the whole tokenized corpus, in a single function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? = padded_everygram_pipeline(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid re-creating the text in memory, both `training_neverygrams` and `padded_vocab_stream` are lazy iterators. They are evaluated on demand at training time.\n",
    "\n",
    "For the sake of understanding the outputs of `padded_everygram_pipeline`, we \"materialize\" the lazy iterators by casting them into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_neverygrams, padded_vocab_stream = padded_everygram_pipeline(2, corp)\n",
    "\n",
    "print('==== n-everygram data (n=2) for each sequence in \"corp\": ====')\n",
    "for ngramlize_sent in training_neverygrams:\n",
    "    print(list(ngramlize_sent))\n",
    "    print()\n",
    "print('==== Vocabulary data: ====')\n",
    "print(list(padded_vocab_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Simpsons Episodes with N-Gram Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4f13ed7359c87e397229104c1bcf18eb20603ad"
   },
   "source": [
    "Let's try some text generation with \"The Simpsons\" TV show episodes!\n",
    "\n",
    "**Dataset source:** https://www.kaggle.com/datasets/prashant111/the-simpsons-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import, inspect and preprocess and tokenize the text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the provided dataset, `simpsons_script_lines.csv`. The `\"spoken_words\"` column gives the desired script lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons = pd.read_csv(\"../data/simpsons_script_lines.csv\",\n",
    "                       usecols=[\"raw_character_text\", \"raw_location_text\", \"spoken_words\", \"normalized_text\"],\n",
    "                       dtype={'raw_character_text':'string', 'raw_location_text':'string',\n",
    "                              'spoken_words':'string', 'normalized_text':'string'})\n",
    "simpsons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that the typical textual dataset is rarely that clean, and that manual text cleaning is typically a required prior step, prior to tokenization and modelling.\n",
    "Some typical cleaning steps e.g. includes: normalizing special characters, like the different types of apostrophes and quotes (e.g. `` ’, ”, ` ``) to the corresponding ` ' ` or ` \" `, remove line breaks `\\n` (careful about not \"merging\" words), and remove multiple spacing. Also having to make make sure urls (e.g. `https://www.website.com/`) are not split into too many meaningless tokens is quite common for social media data. \n",
    "Other types of textual pre-processing/cleaning is typically specific to the dataset and task at hand (some example in future seminars).\n",
    "\n",
    "(Facultative) Feel free to perform cleaning steps that you believe will improve the tokens or the downstream LMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons = simpsons.dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wil train the model to \"talk like\" Homer Simpson. We thus restrict the data to his lines only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons = simpsons[simpsons['raw_character_text']==\"Homer Simpson\"].sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "simpsons.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we tokenize the text corpus into a list of tokenized script lines (documents) by splitting each script line into word tokens. \n",
    "We consider `\"spoken_words\"` and not `\"normalized_text\"`, as we are interested in keeping punctuation and capitalization. \n",
    "The result should be a list of lists containing word-level tokens (e.g. words, punctuation, and other \"special words\"). \n",
    "\n",
    "We use `nltk.word_tokenize`, which is the recommended english tokenizer in `nltk` (model-based).\n",
    "(Alternatives include, `wordpunct_tokenize`, which is a simpler rule-based tokenizer.) \n",
    "You can also use a custom procedure to deal with other data format specifics. \n",
    "We then show the result for the first five script lines of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "\n",
    "print(simpsons['spoken_words'][0])\n",
    "print(word_tokenize(simpsons['spoken_words'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a15d9d1a754ca357ae79e8390069b08b05320458"
   },
   "outputs": [],
   "source": [
    "simpsons_tok = simpsons['spoken_words'].apply(??).to_list()\n",
    "\n",
    "for i in range(5):\n",
    "    print(simpsons_tok[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fitting and Accessing the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk.lm` submodule has implementations of the language models (LM) you have seen in class, and several others. In particular, you will find implementations of: The simple Maximum Likelihood Estimator (MLE) (`nltk.lm.MLE`), Laplace smoothing (`nltk.lm.Laplace`), and Lidstone smoothing (`nltk.lm.Lidstone`). \n",
    "Lidstone is a simple generalization of the other two (more details later).\n",
    "\n",
    "In this section, you will find the very basics on how to use these language model implementations. For more details, you are encouraged to look into the nltk doccumentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fitting an n-gram Language model in NLTK and vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff1ff506e3a14df8283cb93b0a45f86862d3e3c7"
   },
   "source": [
    "Having prepared our data we are ready to start training a model. As a simple example, let us train a Maximum Likelihood Estimator (MLE).\n",
    "\n",
    "We first prepare the itterators for the everygrams and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8724b8724cc52b685205047bd1fda649545a6a24"
   },
   "outputs": [],
   "source": [
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "n = 3\n",
    "training_neverygrams, padded_vocab_stream = padded_everygram_pipeline(n, ??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LM model usage is quite similar to scikit-learn, with an object-oriented implementation. Using the simple MLE as an example, it first has to be instantiated. \n",
    "\n",
    "We only need to specify the highest ngram order to instantiate the MLE (there might be some other hyperparameters for other models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2635458b0f3fed618bfae6a0705ccb11555bfb0b"
   },
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "mle_model = MLE(?) # n is the desired (max) order of the MLE LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "69cdd0e5babaf059a7b9bc87dc0a6e261cef6cb2"
   },
   "source": [
    "Initializing the MLE model, creates an empty vocabulary. The vocabulary object is accessible as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "93b95f89f020585dabfbcd6c41273c823a91e882"
   },
   "outputs": [],
   "source": [
    "len(mle_model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the LM to the training corpus, that has been properly preprocessed into everygrams and a vocabulary stream, for the correct order $n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a31ecc7f6df30a42df9ed6a79039b50f734299cf"
   },
   "outputs": [],
   "source": [
    "# model.fit(training_neverygrams, padded_vocab_stream)\n",
    "mle_model.fit(?, ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb567e332a0089c86e388a8c93f32d1a31737a29"
   },
   "source": [
    "The vocabulary gets filled as the model is fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mle_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6fd7e68c2cdacbe1901fd3b8c05c440706d19100"
   },
   "outputs": [],
   "source": [
    "len(mle_model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74622c58a2df2ac08c8b0c79644d1f87f4014f44"
   },
   "source": [
    "The vocabulary object stores all \"known\" words, and can help handle words that have not occurred during training.\n",
    "One can \"lookup\" a list of tokens in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e5670e34f9d1950f6a1515d57db18092d39484f2"
   },
   "outputs": [],
   "source": [
    "print(mle_model.vocab.lookup(simpsons_tok[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lookup the words of the sentence 'I love UNIGE students!' in the model vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cfec70768e8976073dda05f702f3f210a33dc1c3"
   },
   "outputs": [],
   "source": [
    "# If we lookup the vocab on unseen sentences not from the training data, \n",
    "# it automatically replace words not in the vocabulary with `<UNK>`.\n",
    "print(mle_model.vocab.lookup(word_tokenize(\"I love UNIGE students!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking up the token 'UNIGE' in the model's vocabulary results in the `'<UNK>'` token. This means that this word does not exist in the training corpus. Thus, Homer Simpson sadly never talked about 'UNIGE'..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in ['day', 'food', 'qwertz', 'UNIGE', '<s>', '</s>', '<UNK>']:\n",
    "    print(('Vocabulary contains \\\"' + tok + '\\\": '), (tok in mle_model.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The special token `'<UNK>'` does not appear in the original corpus, neither do the special padding tokens `<s>` and `'</s>'`. Otherwise, it should contain exactly the tokens encountered in the training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2. LM fitting function\n",
    "As `padded_everygram_pipeline` returns itterators (that can only be used once), it is good practice to have the full pipeline in a single function.\n",
    "\n",
    "We thus create a function that takes as arguments (at least) the desired order $n$ of the model and a tokenized training corpus, and that returns the \"simple\" Maximum Likelihood Estimator (MLE) language model, fitted on the given training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ngram_MLE(order, train_corpus_tokens):\n",
    "    \"\"\"\n",
    "    :param order: integer stting the maximum order of the n-grams.\n",
    "    :param train_corpus_tokens: list of tokenized text sequences.\n",
    "    \"\"\"\n",
    "    training_neverygrams, padded_vocab = padded_everygram_pipeline(order=order, text=train_corpus_tokens)\n",
    "    model = nltk.lm.MLE(order=order)\n",
    "    model.fit(training_neverygrams, padded_vocab)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "mle_model = fit_ngram_MLE(order=n, train_corpus_tokens=simpsons_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Accessing the fitted model\n",
    "\n",
    "Apart from the vocabulary, fitting n-gram LMs basically boils down to counting the number of word/token and n-gram occurrences in the training data. To access token counts, and conditional token counts (in a context of one or several preceding tokens), try:\n",
    "```python\n",
    "    model.counts\n",
    "    model.counts['word']\n",
    "    model.counts[('context_word1', \"context_word2\", ...)][\"word\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bfc60d61539269298390044c0d3415bfd28e2b1e"
   },
   "outputs": [],
   "source": [
    "print(mle_model.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9b8cac68de80aee9c7cf7b7f2faf2b199ad27cc"
   },
   "source": [
    "This provides a convenient interface to access counts for unigrams..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90f4611580c41118747a42cfac4c9d729f02523f"
   },
   "outputs": [],
   "source": [
    "mle_model.counts['Marge'] # i.e. Count('Marge') (Marge is Homer's wife)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_model.counts['want'] # i.e. Count('want')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccdf1bf83ba16f0a43f04eb96ca224a968cc81d2"
   },
   "source": [
    "...and bigrams for the phrase bit \"I want\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ba55a83d7c052d1f14923c4046a5f6c86d72c8c"
   },
   "outputs": [],
   "source": [
    "mle_model.counts[['I']]['want'] # i.e. Count('I want')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88bceda0a01e9f2642dd30a6b341c24a606720b9"
   },
   "source": [
    "... and trigrams for the phrase bit \"I want to ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "813e30e1fb153002cfb020d430def2510bf43bbc"
   },
   "outputs": [],
   "source": [
    "mle_model.counts[('I', 'want')]['to'] # i.e. Count('I want a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the real purpose of training a language model is to have it score how probable words are in certain contexts. \n",
    "For the MLE, the model returns the item's relative frequency as its score, i.e. (conditional) occurrence probability.\n",
    "```python\n",
    "    model.score('word')                                             # P('word')\n",
    "    model.score('word', ('context_word1', \"context_word2\", ...))    # P('word'|'context_word1 context_word2 ...')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "051ef5a06c004a8e8ddb6168ca318bc8c0c9abf4"
   },
   "outputs": [],
   "source": [
    "mle_model.score('Marge') # P('Marge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_model.score('want') # P('want')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab3d48919b8f074342e624a1da441621829a4dbd"
   },
   "outputs": [],
   "source": [
    "mle_model.score('want', ('I',))  # P('want'|'I')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_model.score('to', ('I', 'want')) # P('to'|'I want')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e.g. P('Marge') = Counts[('Marge')]/len(vocab)\n",
    "#e.g. P('to'|'I want') = Counts[('I', 'want', 'to')]/Counts[('I', 'want')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: Items that are not seen during training are mapped to a specific vocabulary \"unknown label\" token. The scores for those are 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "44038e8cd8d078b734a6b1a803d0795517e2fa82",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mle_model.score(\"<UNK>\"))\n",
    "print(mle_model.score(\"<UNK>\") == mle_model.score(\"UNIGE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2f170a0bbca4b33b5eb1a55a55f0e1b1b18a634"
   },
   "outputs": [],
   "source": [
    "mle_model.score(\"<UNK>\") == mle_model.score(\"erer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid underflow when working with many small score values it makes sense to take their logarithm. \n",
    "For convenience this can be done by using the `logscore` method instead of the `score`.\n",
    "```python\n",
    "    model.logscore('word')\n",
    "    model.logscore('word', ('context_word1', \"context_word2\", ...))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20ef81b7e161df3b9ab27236c33559713b3077ce"
   },
   "outputs": [],
   "source": [
    "mle_model.logscore('to', ('I', 'want')) # log2(P('to'|'I want'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18f9e0a8d0aba302532b8a84f342d1bf4d3e202a"
   },
   "source": [
    "## 3. Generation using N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Generation with NLTK LMs\n",
    "\n",
    "One cool feature of fitted ngram models is that they can be used to generate text that resembles the training data. The `nltk.lm.model` classes have a `.generate()` method to sample sequentially from the estimated (conditional) probabilities. This can be achieved using:\n",
    "```python\n",
    "    model.generate(num_words = num_words, text_seed = initial_context_tokens, random_seed = None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98989ec4bae592fc98332e759daf9e42bac4213e"
   },
   "outputs": [],
   "source": [
    "print(mle_model.generate(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that this will generate `num_words` new words according to the model's fitted scores, as a list of vocabulary tokens. For a realistic output text, it might thus need some post-processing. `nltk.tokenize.treebank.TreebankWordDetokenizer()` and its `.detokenize()` method provides a general-purpose **sentence** detokenizer, but might need some additional post-processing for specific tasks.\n",
    "\n",
    "Furthermore, are generations without initial context (or text seed) complete examples? Do they look like complete examples similar to the training documents? If not, what is missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mle_model.generate(30, text_seed=['<s>']*(n-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first words generated in the script line should be generated conditionally to the fact that they are the first words of the line. Otherwise, if the unconditional probability is used, a generation could begin with any word from the vocabulary, e.g. in the middle of a sentence. The context (previous tokens) when using a LM of order $n$ should thus be a sequence of $n-1$ start-of-document padding tokens (`'<s>'`, if you did not change the padding default in 2.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b8d07eaf3afae978131573ae127fa0ec3f2e50d"
   },
   "source": [
    "We can do some cleaning and detokenization in a function to make the generated tokens more human-like. In particular it should:\n",
    "- take as input arguments: a fitted `nltk.lm.model`, a maximum number of words (integer), a text seed (initial context tokens), and a random \"RNG\" seed for generation,\n",
    "- have the padding tokens as text seed default, as discussed above,\n",
    "- output a newly generated Simpsons script lines, according to the input arguments, post-processed as a single text string that is formatted like a script line from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73d0d5e0029e64876100e0f2e368b4835a99efcd"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "def simpson_detokenizer(token_list: list[str]) -> str:\n",
    "    TbDetok = TreebankWordDetokenizer()\n",
    "    tb_string = TbDetok.detokenize(token_list)\n",
    "    # As it's a sentence detokenizer, it will add spaces before non-ending punctuation marks:\n",
    "    detokenized_line = tb_string.replace(' .','.').replace(' ,',',').replace(' !','!').replace(' ?','?').replace(' :',':').replace(' ;',';')\n",
    "    # (Possibly more steps depending on pre-processing...)\n",
    "    return detokenized_line\n",
    "\n",
    "def generate_line(model, max_words, text_seed=None, random_seed=None):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param max_words: Max no. of words to generate.\n",
    "    :param text_seed: Generation can be conditioned on preceding context tokens.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    if text_seed is None:\n",
    "        text_seed = ['<s>']*(model.order-1)\n",
    "    \n",
    "    content = [tok for tok in text_seed if tok!='<s>']\n",
    "    \n",
    "    for token in model.generate(num_words=max_words, text_seed=text_seed, random_seed=random_seed):\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        if token != '<s>':\n",
    "            content.append(token)\n",
    "        \n",
    "    line = simpson_detokenizer(content)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate some more realistic Simpsons script lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9041c514e0458236132fe9b3c42ac2ce651beb92"
   },
   "outputs": [],
   "source": [
    "print(mle_model.generate(28, random_seed=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "523e7c2373f6a4c4b6a542f1e82369fedb6cbd21"
   },
   "outputs": [],
   "source": [
    "generate_line(mle_model, max_words=28, text_seed=[], random_seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bce2885bd22d0bb1f79c3861f10fde7df6714e40"
   },
   "outputs": [],
   "source": [
    "generate_line(mle_model, max_words=1000, random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90ddb0e7d0fb52bc77d08425331944c55ee0885e"
   },
   "outputs": [],
   "source": [
    "generate_line(mle_model, max_words=1000, random_seed=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8036f1ae5e0e1438d1d2981026766cdacd4479aa"
   },
   "outputs": [],
   "source": [
    "generate_line(mle_model, max_words=1000, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "defe4e4899cb14300710eb0c7786a6f2e894455d"
   },
   "outputs": [],
   "source": [
    "generate_line(mle_model, max_words=1000, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eba7934efa5a5e6b818e951c5daec9e9f621ca26"
   },
   "outputs": [],
   "source": [
    "generate_line(mle_model, max_words=1000, random_seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ec48d8d44b2f57b16249e12b845f43f52faadca"
   },
   "outputs": [],
   "source": [
    "print(generate_line(mle_model, max_words=1000, random_seed=52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ec48d8d44b2f57b16249e12b845f43f52faadca"
   },
   "outputs": [],
   "source": [
    "print(generate_line(mle_model, max_words=1000, random_seed=17))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83a71e4b25b53795dedce91c1a5686b7e22279e0"
   },
   "source": [
    "**To go further:** Especially with some \"less clean\" data, you could sometimes see in some generations some weird or very particular tokens that probably did not occur often in the training data overall, and that we might want to ignore.\n",
    "\n",
    "For a more advanced usage, the vocabulary can be constructed separately and given to the model, instead of letting it infer it from the vocabulary stream during the model fit. \n",
    "This allows for example cutting-off infrequent words from the vocabulary. \n",
    "You can tell the vocabulary to ignore such words using the `unk_cutoff` argument for the vocabulary lookup, which will turn them to `'<UNK>'`.\n",
    "If you are interested in the implementation and going a bit further, you can check out the documentation for the `nltk.lm.vocabulary.Vocabulary` class [here](https://www.nltk.org/api/nltk.lm.vocabulary.html) or the source code: [`nltk.lm.vocabulary.Vocabulary`](https://github.com/nltk/nltk/blob/develop/nltk/lm/vocabulary.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = nltk.lm.Vocabulary(unk_cutoff=2)\n",
    "voc.update([\"a\",\"b\",\"a\"])\n",
    "voc.lookup([\"a\",\"b\",\"c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc[\"a\"], voc[\"b\"], voc[\"c\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Smoothing and model comparizon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the lecture, the issue of the simple MLE is that it gives 0 probability to any sequence for which even a single trigram has never been seen during the training. To avoid this issue, several smoothing techniques exist. A few implementations are available in the `nltk.lm` submodule, for example:\n",
    "\n",
    " - `Lidstone`: Provides Lidstone-smoothed scores, with hyperparameter $\\gamma$. It avoids the 0 probability issue by adding $\\gamma$ to all counts. A value $\\gamma=0$ corresponds to the simple MLE, and a value $\\gamma=1$ corresponds to Laplace smoothing.\n",
    " - `Laplace`: Implements Laplace (add one) smoothing. It avoids the 0 probability issue by adding $1$ to all counts. Equivalent to Lidstone with $\\gamma=1$.\n",
    " \n",
    " If you want to go further, there are additional language models available in `nltk.lm`.\n",
    " \n",
    "Let's fit the Laplace model introduced in the lecture, as well as its Lindstone generalization, that performs smoothing by adding an arbitrary value `gamma` instead of `1` to the word counts.\n",
    "We can modify the function defined in 2.2., to be compatible with other LMs (and accepts additional hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ngram_language_model(order, train_corpus_tokens, LM_Class=nltk.lm.MLE, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    :param order: integer stting the maximum order of the n-grams.\n",
    "    :param train_corpus_tokens: list of tokenized text sequences.\n",
    "    :param LM_Class: a language model as a nltk.lm.LanguageModel sub-class.\n",
    "    additional arguments are passed to `LM_Class`.\n",
    "    \"\"\"\n",
    "    training_neverygrams, padded_vocab = padded_everygram_pipeline(order=order, text=train_corpus_tokens)\n",
    "    model = LM_Class(order=order, *args, **kwargs)\n",
    "    model.fit(training_neverygrams, padded_vocab)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Laplace, Lidstone\n",
    "\n",
    "laplace = fit_ngram_language_model(??)\n",
    "lidstone = fit_ngram_language_model(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_line(laplace, max_words=1000, text_seed=[\"Marge\", \",\"], random_seed=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_line(lidstone, max_words=1000, text_seed=[\"Marge\", \",\"], random_seed=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Qualitative model comparison "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to observe the impact of the n-gram order on the realism of the generated lines, we can fit and generate new text from the simple MLE and from the Laplace LM of different orders (for ex. $n=1,2,3,4$).\n",
    "- We then compare the results between the different $n$ values and between the two models. \n",
    "- What are the main differences for generation? Which model(s) do you think might be the best options for generating new realistic Homer script lines?\n",
    "- Do you see hints of those differences in the generated text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_models = [None]\n",
    "lapace_models = [None]\n",
    "max_n = 4\n",
    "for n in range(1,max_n+1):\n",
    "    print(n)\n",
    "    mle_models.append(fit_ngram_language_model(order=n, train_corpus_tokens=simpsons_tok, LM_Class=MLE))\n",
    "    lapace_models.append(fit_ngram_language_model(order=n, train_corpus_tokens=simpsons_tok, LM_Class=Laplace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = None\n",
    "prior_tokens = None # [\"Marge\", \",\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==== n=1: ====\")\n",
    "print(generate_line(mle_models[1], 30, text_seed=prior_tokens, random_seed=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==== n=2: ====\")\n",
    "print(generate_line(mle_models[2], 100, text_seed=prior_tokens, random_seed=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==== n=3: ====\")\n",
    "print(generate_line(mle_models[3], 1000, text_seed=prior_tokens, random_seed=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==== n=4: ====\")\n",
    "print(generate_line(mle_models[4], 1000, text_seed=prior_tokens, random_seed=seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==== n=1: ====\")\n",
    "print(generate_line(lapace_models[1], 30, text_seed=prior_tokens, random_seed=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==== n=2: ====\")\n",
    "print(generate_line(lapace_models[2], 100, text_seed=prior_tokens, random_seed=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==== n=3: ====\")\n",
    "print(generate_line(lapace_models[3], 1000, text_seed=prior_tokens, random_seed=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==== n=4: ====\")\n",
    "print(generate_line(lapace_models[4], 1000, text_seed=prior_tokens, random_seed=seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger $n$ values lead to greater sentence coherence, as the model has more context. For even larger $n$, it might also lead to overfitting, and to the model always generating the same sentences from the training set, with little \"novelty\". $n=1$ leads to just random words independently sampled from the dictionary, according to their train corpus frequencies.\n",
    "\n",
    "Laplace might also rarely generate less representative sentences, or sentences with less coherence, as there's always a little probability that the model will generate any word from the vocabulary, regardless of the observed training contexts, due to smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Quantitative model comparison using perplexity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model perplexity is a normalized form of the sequence probability, as seen in the lecture. It can be used on a kept-aside test dataset to evaluate the performance of a ngram probability model. \n",
    "The `nltk.lm.model` classes have a `.perplexity()` method to compute the perplexity on a given list or corpus of n-grams.\n",
    "```python\n",
    "    model.perplexity(test_ngrams)\n",
    "```\n",
    "To compute the perplexity correctly with his method, one needs to preprocess the relevant corpus documents to a list of padded $n$-grams.\n",
    "We can use it to compare the MLE, Laplace and Lindstone (e.g. with $\\gamma=0.1$) models. \n",
    "To do so, we perform the following steps:\n",
    "\n",
    "- Split the tokenized Simpsons lines corpus into a (reproducible) training set (80%) and a test set (20%). \n",
    "- Compute the train and test 3-gram perplexity scores of a simple MLE LM, a Laplace LM, and a Lidstone LM with $\\gamma=0.1$. Use model order $n=3$ for each.\n",
    "- Compare and discuss the obtained train and test perplexity scores of the three models. Argue which model might represent the Homer Simpson script lines data best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_perplexity(lm_model, corpus_tokens, order=None):\n",
    "    if order is None: #Facultative, if you want to evaluate the lm_model for a lower order than its lm_model.order\n",
    "        order=lm_model.order\n",
    "    \n",
    "    test_ngrams = []\n",
    "    for s in corpus_tokens:\n",
    "        test_ngrams += list(ngrams(pad_both_ends(s, n=order), n=order)) #Padded n-grams\n",
    "        \n",
    "    return lm_model.perplexity(test_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_corp, valdid_corp = train_test_split(?, test_size=0.2, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "gamma = 0.1\n",
    "mle3t = fit_ngram_language_model(order=n, train_corpus_tokens=?, LM_Class=?, ?)\n",
    "lapl3t = fit_ngram_language_model(order=n, train_corpus_tokens=?, LM_Class=?, ?)\n",
    "lid3t = fit_ngram_language_model(order=n, train_corpus_tokens=?, LM_Class=?, ?)\n",
    "\n",
    "print(\"Train:\")\n",
    "print(\"MLE:\", evaluate_perplexity(mle3t, ?, n))\n",
    "print(\"Laplace:\", evaluate_perplexity(lapl3t, ?, n))\n",
    "print(f\"Lidstone (gamma={gamma}):\", evaluate_perplexity(lid3t, ?, n))\n",
    "print(\"\")\n",
    "print(\"Test:\")\n",
    "print(\"MLE:\", evaluate_perplexity(mle3t, ?, n))\n",
    "print(\"Laplace:\", evaluate_perplexity(lapl3t, ?, n))\n",
    "print(f\"Lidstone (gamma={gamma}):\", evaluate_perplexity(lid3t, ?, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On train set: The MLE has by far the lowest (i.e. best) and Laplace the largest, with Lidstone in between. This makes sense, as the MLE estimated occurence probabilities are exactly those estimated from the train set, and Laplace and Lidstone smooth the estimated probabilities with \"artificially\" inflated counts, with Lidstone having a lighter smoothing (i.e. counts are inbetween the two others). \n",
    "\n",
    "On the test set: The MLE's perplexity score is infinite. This is generally expected on any set-aside test set, as it suffices to observe a single three-word combination (as $n=3$) that was not present in the training set for the model giving zero probability to the test text. The Laplace and Lidstone smoothing solve this issue with smoothing, by adding $1$ or $\\gamma=0.1$ to all counts, including for (context-conditionally) unobserved words. The Lidstone model has the lowest test perplexity in this case.\n",
    "\n",
    "One can argue that Lidstone has the best tradeoff in the estimated probabilities, as the simple MLE strongly overfits the training data, and Laplace having a significantly largest perplexity, indicating that the $\\gamma=0.1$ might be more suitable than $\\gamma=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the perplexity score as a comparison metric, we can perform a grid-search to select the best values for the hyperparameters $n$ and $\\gamma$ of the Lidstone class of LMs. (Remember the simple MLE and Laplace are spacial cases of lidstone with $\\gamma=0$ and $\\gamma=1$, respectively). \n",
    "The goal is to select the model that generalises best to new data. \n",
    "\n",
    "What do you observe in the obtained perplexity scores? Was it expected? Explain it in statistical terms.\n",
    "\n",
    "- Perform a grid-search to select the best hyperparameter values for $n$ and $\\gamma$, for the Lidstone LM. You want to select the model that generalizes best to new data.\n",
    "- What do you observe in the obtained perplexity scores? Was it expected? Explain it in statistical terms.\n",
    "\n",
    "(One can generally try a few values for $n$ and $\\gamma$ by hand to identify the general hyperparameter region of interest before defining a more thorough hyperparameter value grid.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_list = [0.0001, 0.001, 0.01, 0.1, 0.2]\n",
    "n_list = [1,2,3,4,5]\n",
    "\n",
    "for gamma in gamma_list:\n",
    "    for n in n_list:\n",
    "        lidnt = fit_ngram_language_model(order=?, train_corpus_tokens=?, LM_Class=Lidstone, gamma=?)\n",
    "        print(f\"Lidstone (gamma={gamma}, n={n}):\", evaluate_perplexity(lidnt, ?, n))\n",
    "        # Or clearner: store the values in a a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our grid and data split, the best hyperparameter combination in terms of validation perplexity score seems to be $\\gamma=0.01$ and $n=2$.\n",
    "\n",
    "From that optimum, we observe a \"U-shape\" in both hyperparameter directions. There is a bias-variance tradeoff between low n (bias) and large n (variance) values. Same for the $\\gamma$ values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
