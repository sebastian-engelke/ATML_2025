{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Walk: TD vs MC comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we compare the performance of temporal difference (TD(0)) and Monte Carlo.\n",
    "The example we consider is a so-called Markov reward process,\n",
    "i.e. a Markov decision process without actions.\n",
    "\n",
    "For a more detailed description see Example 6.2, page 125, in Sutton & Barto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is a finite, integer random walk of a given length,\n",
    "with the agent starting in the middle of the way.\n",
    "\n",
    "Each step they go left or right with equal probability.\n",
    "\n",
    "The two outermost states (`0` and `length-1`) are terminal.\n",
    "\n",
    "When the agent enters state `length-1` they receive a reward of +1, anytime else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RandomWalk:\n",
    "    def __init__(self, length):\n",
    "        ... #??\n",
    "    \n",
    "    def step(self):\n",
    "        ... #??\n",
    "    \n",
    "    def reset(self):\n",
    "        ... #??\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the length of the random walk used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH = 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance and test the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "... #??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement Monte Carlo prediction, roughly following the pseudo-code on page 114 of [Sutton & Barto](http://incompleteideas.net/book/RLbook2020trimmed.pdf#page=114).\n",
    "The algorithm is changed by using a constant update rate $\\alpha$, rather than averaging over all previous returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize:\n",
    "# V(s) <- arbitrarily\n",
    "\n",
    "# Loop forever (for each episode):\n",
    "    # Generate an episode: S0, A0, R1, S1, A1, R2, ..., ST-1, AT-1, RT\n",
    "    # G <- 0\n",
    "    # Loop for each step of episode, t = T-1, T-2, ..., 0:\n",
    "        # G <- GAMMA * G + Rt+1\n",
    "        # Unless St appears in S0, A0, S1, A1, ..., St-1, At-1:\n",
    "            # V(St) <- V(St) + alpha * (G - V(St))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a helper function to generate an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEpisode(randomWalk: RandomWalk):\n",
    "    ... #??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the code for a single episode of MC.\n",
    "The value function is passed as `values` and updated in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def monteCarloEpisode(randomWalk: RandomWalk, values, alpha):\n",
    "    ... #??\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we perform MC by running many episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform MC by running many episodes\n",
    "N_EPISODES = 10000\n",
    "ALPHA = 0.02\n",
    "\n",
    "... #??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TD(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the code for a single episode of TD(0).\n",
    "The value function is passed as `values` and updated in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TD(0) to estimate value function V\n",
    "\n",
    "# Algorithm parameter alpha\n",
    "# Initialize V(s) such that V(terminal)=0\n",
    "# for each episode do:\n",
    "    # Initialize S\n",
    "    # repeat:\n",
    "        # Take step, observe R, S'\n",
    "        # V(S) <- V(S) + alpha * (R + V(S') âˆ’ V (S))\n",
    "        # S <- S'\n",
    "    # until S is terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tdEpisode(randomWalk: RandomWalk, values, alpha):\n",
    "    ... #??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform TD(0) by running many episodes\n",
    "... #??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the true value of state $s$ is $s / (LENGTH - 1)$.\n",
    "We use this to compare the error made by each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_VALUES = [i / (LENGTH - 1) for i in range(LENGTH)]\n",
    "\n",
    "def computeRMS(values):\n",
    "    errors = [v - t for v, t in zip(values[1:-1], TRUE_VALUES[1:-1])]\n",
    "    return np.sqrt(np.mean(np.square(errors)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we recreate the left graph from p. 125, Sutton & Barto, showing the value function for different numbers of episodes $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotValueSteps(showN, alpha):\n",
    "    values = [0.5] * LENGTH\n",
    "    values[0] = 0\n",
    "    values[-1] = 0\n",
    "\n",
    "    randomWalk = RandomWalk(LENGTH)\n",
    "    \n",
    "    n = np.max(showN) + 1\n",
    "    \n",
    "    # Perform TD, plotting the values of steps in showN\n",
    "    for i in range(n):\n",
    "        if i in showN:\n",
    "            plt.plot(values[1:-1], label = 'n = ' + str(i))\n",
    "        # tdEpisode(randomWalk, values, alpha)\n",
    "        monteCarloEpisode(randomWalk, values, alpha)\n",
    "    \n",
    "    plt.plot(TRUE_VALUES[1:-1], linestyle='dashdot', label=\"true values\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotValueSteps([0, 1, 10, 100], 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function that performs several TD/MC episodes and computes the RMS after each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def computeRMSs(alpha, nEpisodes, xxxEpisode):\n",
    "    randomWalk = RandomWalk(LENGTH)\n",
    "    rms = []\n",
    "    values = [0.5] * LENGTH\n",
    "    values[0] = 0\n",
    "    values[-1] = 0\n",
    "    for j in range(nEpisodes):\n",
    "        xxxEpisode(randomWalk, values, alpha)\n",
    "        rms.append(computeRMS(values))\n",
    "    return rms\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function above,\n",
    "we plot the RMS vs. the number of training episodes for both methods and different values of `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compared alphas\n",
    "alphasMC = [0.01, 0.02, 0.03, 0.04]\n",
    "alphasTD = [0.01, 0.05, 0.10, 0.20]\n",
    "\n",
    "# Number of episodes per experiment\n",
    "nEpisodes = 200\n",
    "\n",
    "# Number of experiments\n",
    "nExperiments = 100\n",
    "\n",
    "print('MC...')\n",
    "for i, alpha in enumerate(alphasMC):\n",
    "    print(alpha)\n",
    "    rmsList = [computeRMSs(alpha, nEpisodes, monteCarloEpisode) for _ in range(nExperiments)]\n",
    "    rms = np.mean(rmsList, axis=0)\n",
    "    plt.plot(rms, linestyle='dashdot', label='MC {}'.format(alpha))\n",
    "\n",
    "print('TD...')\n",
    "for i, alpha in enumerate(alphasTD):\n",
    "    print(alpha)\n",
    "    rmsList = [computeRMSs(alpha, nEpisodes, tdEpisode) for _ in range(nExperiments)]\n",
    "    rms = np.mean(rmsList, axis=0)\n",
    "    plt.plot(rms, linestyle='solid', label='TD {}'.format(alpha))\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
