{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8268bc47",
   "metadata": {},
   "source": [
    "# NLP Seminar 5: Pretrained Transformers and Transfer-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48722a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efa03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses, optimizers\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1655fb0",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16eb59",
   "metadata": {},
   "source": [
    "Transformers can be implemented from scratch in both tensorflow and pytorch\n",
    "(e.g. https://www.tensorflow.org/text/tutorials/transformer).\n",
    "The multi-headed attention layers used in transformers are also implemented as a Keras layer in tensorflow ([Attention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention), [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention)).\n",
    "However, constructing or reproducing meaningful transformer architectures from scratch, even with these building blocks, can still remain challenging. This is especially true for some of the more complex MLP tasks, combining encoder and decoder transformers.\n",
    "Furthermore, transformers have really proved their state-of-the art efficiency for NLP tasks when trained on huge corpora of data. In particular, for many specific tasks, transfer learning is used to leverage the dynamic semantic information already acquired by pre-trained models.\n",
    "\n",
    "Although transfer-learning using pre-trained transformers such as BERT is possible with tensorflow (e.g. [classify_text_with_bert](https://www.tensorflow.org/text/tutorials/classify_text_with_bert), [fine_tune_bert](https://www.tensorflow.org/tfmodels/nlp/fine_tune_bert)) this practical will instead introduce the `HuggingFace` transformer library, as it\n",
    "- has a lot of pretrained state-of-the-art transformer models for various tasks,\n",
    "- has a very high-level user-friendly interface,\n",
    "- is compatible with both tensorflow and pytorch,\n",
    "- is used by many universities, research labs and companies.\n",
    "\n",
    "If needed, see the official tutorials to go further:\n",
    "- https://huggingface.co/learn/llm-course/chapter1/1\n",
    "- https://huggingface.co/docs/transformers/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25780041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2f129e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\pascheo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In case of version incompatibility issues between transformers and TensorFlow::\n",
    "# !pip install tf-keras\n",
    "from tf_keras import layers, losses, optimizers, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec86e9",
   "metadata": {},
   "source": [
    "## Pre-trained pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608bf97",
   "metadata": {},
   "source": [
    "The `pipeline` allows loading pre-trained models with a very easy interface, for a wide range of different tasks from the `HuggingFace` database. Almost all main open-source pretrained transformer references (BERT, GPT, ...) are available.\n",
    "\n",
    "- Popular transformer architectures: https://huggingface.co/docs/transformers/v4.49.0/en/index\n",
    "- community checkpoints: https://huggingface.co/models\n",
    "\n",
    "Here are a few examples of pretrained transformer models (i.e. checkpoints) for some NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15ba0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc1d6e",
   "metadata": {},
   "source": [
    "#### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07cf0b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sent_pipe = pipeline(\"sentiment-analysis\", # or \"text-classification\"\n",
    "                     model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb7887f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997754693031311}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_pipe(\"The weather is bad today...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6eb31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: NEGATIVE, with score: 0.9998\n",
      "label: POSITIVE, with score: 0.9998\n"
     ]
    }
   ],
   "source": [
    "results = sent_pipe([\"The weather is bad today...\",\n",
    "                     \"It is sunny and warm outside.\"])\n",
    "\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e41bb",
   "metadata": {},
   "source": [
    "#### \"Zero-shot\" classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b11e53",
   "metadata": {},
   "source": [
    "Using natural language inference models to predict *entailment* between each sequence-label premise/hypothesis pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0244c3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "zsc_pipe = pipeline('zero-shot-classification',\n",
    "                    model=\"facebook/bart-large-mnli\", revision=\"c626438\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33dfde41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I like trains.',\n",
       " 'labels': ['vehicles', 'animals', 'politics'],\n",
       " 'scores': [0.9911500811576843, 0.004934567026793957, 0.003915321547538042]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zsc_pipe(\"I like trains.\", [\"politics\",\"vehicles\",\"animals\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbfb58",
   "metadata": {},
   "source": [
    "#### Dynamic word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6c9d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "dwe_pipe = pipeline(\"feature-extraction\", model=\"bert-base-cased\") # e.g. \"bert-base-cased\" \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14f31052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8597,  0.1216, -0.0761,  ..., -0.1833,  0.1758,  0.0822],\n",
       "         [ 0.8721, -0.3453,  0.5372,  ..., -0.1697,  0.0987,  0.1806],\n",
       "         [ 0.6673, -0.0972, -0.5464,  ...,  0.5048, -0.4832,  0.1718],\n",
       "         [ 0.4819, -0.0781,  0.1035,  ...,  0.0943, -0.3238,  0.1974],\n",
       "         [ 0.8397, -0.1973, -0.0363,  ..., -0.0951,  0.3160, -0.0663],\n",
       "         [ 1.6680,  0.1131, -0.2623,  ..., -0.2753,  0.5270, -0.1521]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dwe_pipe(\"I like trains.\", return_tensors=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba10a4",
   "metadata": {},
   "source": [
    "This outputs the last transformer block output. Some other embedding approaches exist, like averaging or concatenating the activations of several of the transformer's layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873fee0",
   "metadata": {},
   "source": [
    "#### Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf77c74",
   "metadata": {},
   "source": [
    "With causal language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62a002eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\") #\"HuggingFaceTB/SmolLM2-360M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3050884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this NLP seminar about transformers, we will learn about transformation and how to use them effectively. It also covers how they can increase performance in many more tasks over the next few weeks as we improve our methods and tools in our own way.'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"In this NLP seminar about transformers, we will learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7560a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this NLP seminar about transformers, we will learn about them, what they are, and how we can apply them to create solutions.\\n'},\n",
       " {'generated_text': 'In this NLP seminar about transformers, we will learn about the different ways in which they can be used and a lot more. Our approach is'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"In this NLP seminar about transformers, we will learn\",\n",
    "          max_length=30,\n",
    "          num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4561cc6",
   "metadata": {},
   "source": [
    "#### Mask filling\n",
    "This language model task is part of how BERT architectures are often pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b48250e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82977257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.18968380987644196,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This seminar will teach you all about mathematical models.'},\n",
       " {'score': 0.04678095877170563,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This seminar will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"This seminar will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2cb8e0",
   "metadata": {},
   "source": [
    "#### Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "565f3924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "c:\\Users\\pascheo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad3160a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9992693,\n",
       "  'word': 'Olivier',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.97970057,\n",
       "  'word': 'University of Geneva',\n",
       "  'start': 37,\n",
       "  'end': 57},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9608657,\n",
       "  'word': 'Plainpalais',\n",
       "  'start': 63,\n",
       "  'end': 74}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(\"My name is Olivier and I work at the University of Geneva near Plainpalais.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129be83d",
   "metadata": {},
   "source": [
    "#### Question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75629f",
   "metadata": {},
   "source": [
    "Using extractive encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d12b4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bcd89ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.7924377918243408,\n",
       " 'start': 37,\n",
       " 'end': 57,\n",
       " 'answer': 'University of Geneva'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer(question=\"Where do I work?\",\n",
    "                  context=\"My name is Olivier and I work at the University of Geneva near Plainpalais. I have a lot of work at the office this week.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234865c5",
   "metadata": {},
   "source": [
    "#### Document summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858893d",
   "metadata": {},
   "source": [
    "Using encoder-decoder transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02d90576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5eace5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science, and artificial intelligence . The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them . The technology can then extract information and insights from documents as well as categorize and organize the documents themselves .'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\n",
    "    # Wikipedia page on NLP:\n",
    "    \"\"\"\n",
    "    Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science,\n",
    "    and artificial intelligence concerned with the interactions between computers and human language,\n",
    "    in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "    The goal is a computer capable of \"understanding\" the contents of documents, including the contextual\n",
    "    nuances of the language within them. The technology can then accurately extract information and insights\n",
    "    contained in the documents as well as categorize and organize the documents themselves.\n",
    "    \n",
    "    Challenges in natural language processing frequently involve speech recognition, natural-language\n",
    "    understanding, and natural-language generation.\n",
    "    \n",
    "    Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article\n",
    "    titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a\n",
    "    criterion of intelligence, though at the time that was not articulated as a problem separate from\n",
    "    artificial intelligence. The proposed test includes a task that involves the automated interpretation\n",
    "    and generation of natural language.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29568d5",
   "metadata": {},
   "source": [
    "#### Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276730d4",
   "metadata": {},
   "source": [
    "Using encoder-decoder transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46e77fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pascheo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e699010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This seminar is given every week on Thursday afternoon.'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(\"Ce séminaire est donné chaque semaine le jeudi après-midi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0214736a",
   "metadata": {},
   "source": [
    "Or the more recent multilingual type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "051ac792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pascheo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1156: UserWarning: \"translation\" task was used, instead of \"translation_XX_to_YY\", defaulting to \"translation_en_to_de\"\n",
      "  warnings.warn(\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "mtranslator = pipeline(task=\"translation\", model=\"google-t5/t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a809b65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Je voudrais en savoir plus sur les LLM.'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtranslator(\"translate to French: I would like to learn more about LLMs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fed091",
   "metadata": {},
   "source": [
    "## What constitues a pipeline?\n",
    "\n",
    "Example for a classification pipeline.\n",
    "\n",
    "https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31af0f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "pretrained_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "sent_pipe = pipeline(\"sentiment-analysis\", model=pretrained_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6aef063",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = [\"I love this amazing Transformers introduction seminar.\",\n",
    "        \"I hate debugging my code so much!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7ed7854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998568296432495},\n",
       " {'label': 'NEGATIVE', 'score': 0.9962984919548035}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_pipe(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525e26c",
   "metadata": {},
   "source": [
    "### Step 1. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae42545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98f5e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5ba81d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([[  101,  1045,  2293,  2023,  6429, 19081,  4955, 18014,  1012,\n",
      "          102,     0,     0],\n",
      "       [  101,  1045,  5223,  2139,  8569, 12588,  2026,  3642,  2061,\n",
      "         2172,   999,   102]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary BoW indexes for the tokenized text corpus:\n",
    "inputs = tokenizer(corp, padding=True, truncation=True, return_tensors=\"np\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c401a7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[  101,  1045,  2293,  2023,  6429, 19081,  4955, 18014,  1012,\n",
       "          102,     0,     0],\n",
       "       [  101,  1045,  5223,  2139,  8569, 12588,  2026,  3642,  2061,\n",
       "         2172,   999,   102]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For comparison, the pipeline tokenizer is the same:\n",
    "sent_pipe.tokenizer(corp, padding=True, truncation=True, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5b594fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'love', 'this', 'amazing', 'transformers', 'introduction', 'seminar', '.', '[SEP]']\n",
      "['[CLS]', 'i', 'hate', 'de', '##bu', '##gging', 'my', 'code', 'so', 'much', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#Textual version of the tokens:\n",
    "for doc in corp:\n",
    "    print(tokenizer.tokenize(doc, add_special_tokens=True)) # sub-word / wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8310c05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i love this amazing transformers introduction seminar. [SEP] [PAD] [PAD]'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 1045, 2293, 2023, 6429, 19081, 4955, 18014, 1012, 102, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72c406fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i hate debugging my code so much! [SEP]'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 1045, 5223, 2139, 8569, 12588, 2026, 3642, 2061, 2172, 999, 102])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5456505b",
   "metadata": {},
   "source": [
    "### Step 2.1. Transformer model\n",
    "\n",
    "https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40618403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4ecc3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\pascheo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(pretrained_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c58be445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66362880 (253.15 MB)\n",
      "Trainable params: 66362880 (253.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "150157a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutput(last_hidden_state=<tf.Tensor: shape=(2, 12, 768), dtype=float32, numpy=\n",
       "array([[[ 0.7409617 ,  0.09954516,  0.17740384, ...,  0.37085018,\n",
       "          1.0475554 , -0.53418255],\n",
       "        [ 0.95344263,  0.1784999 ,  0.07076187, ...,  0.35534364,\n",
       "          1.1498725 , -0.32719424],\n",
       "        [ 1.1049625 ,  0.25522926,  0.33767354, ...,  0.3171588 ,\n",
       "          1.0195712 , -0.3793204 ],\n",
       "        ...,\n",
       "        [ 1.1075132 ,  0.08023884,  0.68834203, ...,  0.6212788 ,\n",
       "          0.59678215, -0.804865  ],\n",
       "        [ 0.5849833 ,  0.11724682,  0.09631411, ...,  0.5941564 ,\n",
       "          1.0606879 , -0.2870518 ],\n",
       "        [ 0.5737111 ,  0.0819948 ,  0.07770069, ...,  0.43690968,\n",
       "          1.0691311 , -0.36507562]],\n",
       "\n",
       "       [[-0.12865382,  0.6125046 , -0.43362278, ...,  0.02990168,\n",
       "         -0.37186378,  0.16652936],\n",
       "        [-0.08211732,  0.91229683, -0.15405825, ..., -0.02132722,\n",
       "         -0.2596661 ,  0.25903916],\n",
       "        [-0.05406391,  0.6817771 , -0.06585187, ...,  0.07498465,\n",
       "         -0.32770908,  0.12720081],\n",
       "        ...,\n",
       "        [-0.39177427,  0.50210375, -0.42975858, ...,  0.04450966,\n",
       "         -0.8621887 , -0.03975022],\n",
       "        [ 0.13117795,  0.8358263 , -0.5016037 , ...,  0.1448584 ,\n",
       "         -0.31037566,  0.04740491],\n",
       "        [ 0.21348679,  0.5011953 , -0.22045526, ..., -0.03712649,\n",
       "         -0.5186229 , -0.09755841]]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8821b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 12, 768)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce0df1",
   "metadata": {},
   "source": [
    "### Step 2.2. Transformer model with classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac6e3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5529e855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "classif_model = TFAutoModelForSequenceClassification.from_pretrained(pretrained_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac20a4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66955010 (255.41 MB)\n",
      "Trainable params: 66955010 (255.41 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classif_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33a8b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2 = classif_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b7b5c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-4.2869983,  4.5641413],\n",
       "       [ 3.06056  , -2.5347545]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8e358a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-4.2869983,  4.5641413],\n",
       "       [ 3.06056  , -2.5347545]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs2.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea35fd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(outputs2.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e20c4",
   "metadata": {},
   "source": [
    "### Step 3. Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "285d374f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1.4319799e-04, 9.9985683e-01],\n",
       "       [9.9629849e-01, 3.7014787e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = tf.keras.activations.softmax(outputs2.logits, axis=-1)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba8da367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted class:\n",
    "probabilities.numpy().argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d51eca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9998568, 0.9962985], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction certainty:\n",
    "probabilities.numpy().max(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f4d1cb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class labels corresponding to the integer incoding:\n",
    "classif_model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf41a662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998568296432495},\n",
       " {'label': 'NEGATIVE', 'score': 0.9962984919548035}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For comparison, the pipeline output:\n",
    "sent_pipe(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd67700",
   "metadata": {},
   "source": [
    "For different tasks, there might be additional preprocessing and feature extraction steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be7e01",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "298bbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and tokenizer\n",
    "tf_save_directory = \"./checkpoints/tf_save_pretrained\"\n",
    "tokenizer.save_pretrained(tf_save_directory)\n",
    "classif_model.save_pretrained(tf_save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d1b218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./checkpoints/tf_save_pretrained were not used when initializing TFDistilBertForSequenceClassification: ['dropout_38']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ./checkpoints/tf_save_pretrained and are newly initialized: ['dropout_58']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "classif_model = TFAutoModelForSequenceClassification.from_pretrained(\"./checkpoints/tf_save_pretrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e0587e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./checkpoints/tf_save_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966fdfcf",
   "metadata": {},
   "source": [
    "## Transfer learning with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f618b778",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f996876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpsons = pd.read_csv(\"../data/simpsons_script_lines.csv\",\n",
    "                       usecols=[\"raw_character_text\", \"raw_location_text\", \"spoken_words\", \"normalized_text\"],\n",
    "                       dtype={'raw_character_text':'string', 'raw_location_text':'string',\n",
    "                              'spoken_words':'string', 'normalized_text':'string'})\n",
    "simpsons = simpsons.dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "n_classes = 10\n",
    "main_characters = simpsons['raw_character_text'].value_counts(dropna=False)[:n_classes].index.to_list()\n",
    "simpsons_main = simpsons.query(\"`raw_character_text` in @main_characters\")\n",
    "\n",
    "X = simpsons_main[\"normalized_text\"].to_numpy()\n",
    "y = simpsons_main[\"raw_character_text\"].to_numpy()\n",
    "y_int = np.array([np.where(np.array(main_characters)==char)[0].item() for char in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b2c6f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y_int, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00668b08",
   "metadata": {},
   "source": [
    "#### Loading the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "607dd1f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "pretrained_name2 = \"bert-base-cased\" # e.g. \"bert-base-cased\" or \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_name2)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(pretrained_name2, num_labels=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d3afb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      " dropout_96 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  7690      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108317962 (413.20 MB)\n",
      "Trainable params: 108317962 (413.20 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead0f6e",
   "metadata": {},
   "source": [
    "To freeze the pretrained transformer weights, and only train the classification head, we can set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "934802ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec55f7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      " dropout_96 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  7690      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108317962 (413.20 MB)\n",
      "Trainable params: 7690 (30.04 KB)\n",
      "Non-trainable params: 108310272 (413.17 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a635333",
   "metadata": {},
   "source": [
    "Allowing the transformer weights to be modified by leaving `model.layers[0].trainable = True` can significantly improve performance of the downstream task, but will take significantly longer to train. Furthermore, more care needs to be taken when selecting the training hyperparameters (low initial learning rate, learning rate decay, not too many epochs), to prevent [Catastrophic forgetting](https://en.wikipedia.org/wiki/Catastrophic_interference), and loosing pretraining information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b4149286",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tok = dict(tokenizer(X_train.tolist(), padding=True, truncation=True, return_tensors=\"tf\"))\n",
    "X_valid_tok = dict(tokenizer(X_valid.tolist(), padding=True, truncation=True, return_tensors=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5e2355cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(learning_rate=3e-5),\n",
    "              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc3830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fit is very long without a GPU or a cloud service\n",
    "epochs = 5\n",
    "history_ft = model.fit(X_train_tok, y_train, validation_data=(X_valid_tok, y_valid),\n",
    "                       batch_size=16, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dc9db1",
   "metadata": {},
   "source": [
    "For larger dataset sizes, one can perform more efficient training using HuggingFace's `Datasets`, that can allow for smarter parallel memory allocation from disk: https://huggingface.co/docs/datasets/index.\n",
    "\n",
    "HuggingFace's `Datasets` can then also interact with the Keras API (e.g. the `model.fit()` method), for example through `model.prepare_tf_dataset()` or `Dataset.to_tf_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089029cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
