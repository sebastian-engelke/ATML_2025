{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA and Q-Learning: Cliffworld Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we use the cliffworld example to implement SARSA and Q-Learning,\n",
    "and to illustrate how their behavior differs.\n",
    "\n",
    "For a detailed description of the setting see\n",
    "[Example 6.6 in *Sutton & Barto*](http://incompleteideas.net/book/RLbook2020trimmed.pdf#page=154)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gridworld import GridWorld \n",
    "from collections import defaultdict\n",
    "\n",
    "# Use `tk` for interactive play, `inline` for normal plots:\n",
    "# %matplotlib tk\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we recreate the setting from the example using our generic gridworld class.\n",
    "\n",
    "*(How this is done is somewhat specific to our implementation and the details do not matter too much.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dimension and start/goal squares\n",
    "HEIGHT = 4\n",
    "WIDTH = 12\n",
    "START = (HEIGHT - 1, 0)\n",
    "GOAL = (HEIGHT - 1, WIDTH - 1)\n",
    "\n",
    "# Create (empty) gridworld instance\n",
    "gw = GridWorld(HEIGHT, WIDTH, START)\n",
    "\n",
    "# Label start/goal\n",
    "gw.positionLabels[START] = 'S'\n",
    "gw.positionLabels[GOAL] = 'G'\n",
    "\n",
    "# Add cliffs to the bottom row (except for corners)\n",
    "for i in range(1, WIDTH - 1):\n",
    "    gw.immediateTeleportations[(HEIGHT - 1, i)] = (START, -200)\n",
    "\n",
    "# Make goal state terminal\n",
    "gw.teleportations[GOAL] = (GOAL, 0)\n",
    "\n",
    "# Give -1 reward for all other transitions\n",
    "gw.rewards[START] = -1\n",
    "gw.invalidActionReward = -1\n",
    "for i in range(HEIGHT - 1):\n",
    "    for j in range(WIDTH):\n",
    "        gw.rewards[(i, j)] = -1\n",
    "\n",
    "# A small chance of doing something random, just to make a point\n",
    "gw.randomChance = 0.0\n",
    "\n",
    "# Helper list, containing all possible states and actions\n",
    "ALL_STATES = list((i, j) for i in range(gw.height) for j in range(gw.width))\n",
    "ACTIONS = [0, 1, 2, 3]\n",
    "\n",
    "# Plot the world\n",
    "plt.figure(figsize=(10,3))\n",
    "gw.drawWorld()\n",
    "plt.show()\n",
    "\n",
    "# # Play in the world\n",
    "# gw.play()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some parameters used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "ALPHA = 0.1\n",
    "\n",
    "# Exploration rate\n",
    "EPSILON = 0.1\n",
    "\n",
    "# Discount factor\n",
    "GAMMA = 1.0\n",
    "\n",
    "# Number of episodes in SARSA/Q-learning\n",
    "N_EPISODES = 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we implement SARSA to solve the cliff world environment.\n",
    "\n",
    "We start by implementing a helper function that lets us choose an action in an epsilon-greedy manner, according to our current estimate of the state-action values $q(s,a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseEpsilonGreedy(allQValues, state, eps):\n",
    "    # Choose random action with probability `eps`\n",
    "    # ...\n",
    "    \n",
    "    # Chose optimal action (breaking ties randomly)\n",
    "    # ...\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement SARSA itself, following the pseudocode in *Sutton & Barto*, page 130."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q(s,a) arbitrarily\n",
    "# Repeat (for each episode):\n",
    "#     Initialize s\n",
    "#     Choose a from s using policy derived from Q (e.g., epsilon-greedy)\n",
    "#     Repeat (for each step of episode):\n",
    "#         Take action a, observe r, s'\n",
    "#         Choose a' from s' using policy derived from Q (e.g., epsilon-greedy)\n",
    "#         Q(s,a) <- Q(s,a) + alpha * [r + gamma * Q(s',a') - Q(s,a)]\n",
    "#         s <- s'; a <- a';\n",
    "#     until s is terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsaEpisode(world: GridWorld, qValues):\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q(s,a) arbitrarily\n",
    "# ...\n",
    "\n",
    "# Repeat (for each episode):\n",
    "# ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we illustrate the result of SARSA by looking at the paths taken during training, the estimated value function, and the corresponding optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot paths taken at various stages\n",
    "plt.figure(figsize=(12, 3))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "# ...\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "# ...\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "# ...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimal policy and state values\n",
    "policy = dict()\n",
    "values = dict()\n",
    "\n",
    "# Compute policy/value for each state\n",
    "# ...\n",
    "\n",
    "# Plot policy\n",
    "plt.figure(figsize=(10,3))\n",
    "gw.drawWorld(drawRewards=False, policy=policy)\n",
    "plt.show()\n",
    "\n",
    "# Plot values\n",
    "plt.figure(figsize=(10,3))\n",
    "gw.drawWorld(drawRewards=False, values=values)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more basic way to plot the policy\n",
    "labels = ['U', 'R', 'D', 'L']\n",
    "for i in range(gw.height):\n",
    "    for j in range(gw.width):\n",
    "        # ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to play and plot a greedy game (w.r.t. the supplied Q-values)\n",
    "def illustrateGreedyGame(world: GridWorld, qValues):\n",
    "    world.reset()\n",
    "    states = [world.pos]\n",
    "    action = chooseEpsilonGreedy(qValues, world.pos, 0.0)\n",
    "    actions = [action]\n",
    "    rewards = [0]\n",
    "    while world.pos != GOAL:\n",
    "        newPos, reward = world.step(action)\n",
    "        action = chooseEpsilonGreedy(qValues, world.pos, 0.0)\n",
    "        rewards.append(reward)\n",
    "        states.append(world.pos)\n",
    "        actions.append(action)\n",
    "    plt.figure(figsize=(10,3))\n",
    "    gw.drawWorld(drawRewards=False, path=states)\n",
    "    plt.show()\n",
    "    print('Total reward:', sum(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrateGreedyGame(gw, sarsaQValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how often each state was visited\n",
    "sarsaHitCounts = defaultdict(lambda: 0)\n",
    "# ...\n",
    "\n",
    "# Plot hit counts\n",
    "plt.figure(figsize=(12, 3))\n",
    "# ...\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we implement Q-learning to solve the cliff world environment, \n",
    "following the pseudocode in *Sutton & Barto*, page 131."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q(s,a) arbitrarily\n",
    "# Repeat (for each episode):\n",
    "#     Initialize s\n",
    "#     Repeat (for each step of episode):\n",
    "#         Choose a from s using policy derived from Q (e.g., epsilon-greedy)\n",
    "#         Take action a, observe r, s'\n",
    "#         Q(s,a) <- Q(s,a) + alpha * [r + gamma * max_a' Q(s',a') - Q(s,a)]\n",
    "#         s <- s';\n",
    "#     until s is terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearningEpisode(world: GridWorld, qValues):\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q(s,a) arbitrarily\n",
    "# ...\n",
    "\n",
    "# Repeat (for each episode):\n",
    "# ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the results of Q-learning similarly to the results of SARSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot paths taken at various stages\n",
    "plt.figure(figsize=(12,3))\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "# ...\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "# ...\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "# ...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the optimal policy and state values\n",
    "policy = dict()\n",
    "values = dict()\n",
    "\n",
    "# Plot the optimal policy and state values\n",
    "# ...\n",
    "\n",
    "# Plot policy\n",
    "plt.figure(figsize=(10,3))\n",
    "gw.drawWorld(drawRewards=False, policy=policy)\n",
    "plt.show()\n",
    "\n",
    "# Plot values\n",
    "plt.figure(figsize=(10,3))\n",
    "gw.drawWorld(drawRewards=False, values=values)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['U', 'R', 'D', 'L']\n",
    "for i in range(gw.height):\n",
    "    for j in range(gw.width):\n",
    "        # ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the illustrate function from above\n",
    "# illustrateGreedyGame(gw, qLearningQValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how often each state was visited\n",
    "qLearningHitCounts = defaultdict(lambda: 0)\n",
    "# ...\n",
    "\n",
    "# Plot hit counts\n",
    "plt.figure(figsize=(12, 3))\n",
    "# ...\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of per-episode rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compare the per-episode rewards of SARSA and Q-learning.\n",
    "Plotting a moving average might be more insightful than raw rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute a moving average\n",
    "def movingAverage(a, k):\n",
    "    return np.convolve(a, np.ones(k), 'valid') / k\n",
    "\n",
    "# Plot (moving average) of per-episode rewards\n",
    "N_AVERAGE = 100\n",
    "plt.figure()\n",
    "# ...\n",
    "plt.show()\n",
    "\n",
    "# Compute average over the second half of episodes\n",
    "print('Long-term average rewards\\n---')\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
